{"config":{"lang":["en"],"separator":"[\\s\\-]+"},"docs":[{"title":"Welcome to the DataJoint Documentation","text":"DataJoint Ecosystem   <ul> <li>DataJoint Core <p>Open-source clients for managing data pipelines.</p> <p>- DataJoint API for Python</p> <p>- DataJoint API for MATLAB</p> <p>- DataJoint Pharus</p> <p>- DataJoint LabBook</p> <p>- DataJoint SciViz</p> <p>- DataJoint Container Images</p> <p> Learn more</p> </li> </ul> <ul> <li>DataJoint Elements <p>Open-source data pipelines for neuroscience built with DataJoint API for Python.</p> <p> Learn more</p> </li> </ul> <ul> <li>DataJoint Works <p>Managed services for neurophysiology built with DataJoint Core and DataJoint Elements.</p> <p> Learn more</p> </li> </ul>   Research supported by DataJoint   <ul> <li>Projects <p> Learn more</p> </li> </ul> <ul> <li>Publications <p> Learn more</p> </li> </ul>","location":""},{"title":"Citation","text":"<p>If your work uses the DataJoint APIs and DataJoint Elements, please cite the respective Research Resource Identifiers (RRIDs) and manuscripts.</p>","location":"citation/"},{"title":"DataJoint API for Python or MATLAB","text":"<p>Manuscript:</p>  <p>Yatsenko D, Reimer J, Ecker AS, Walker EY, Sinz F, Berens P, Hoenselaar A, Cotton RJ, Siapas AS, Tolias AS. DataJoint: managing big scientific data using MATLAB or Python. bioRxiv. 2015 Jan 1:031658. doi: https://doi.org/10.1101/031658</p>  <p>RRID:</p>  <p>DataJoint (RRID:SCR_014543) - DataJoint <code>&lt;Select Python or MATLAB&gt;</code> (version <code>&lt;Enter version number&gt;</code>)</p>","location":"citation/#datajoint-api-for-python-or-matlab"},{"title":"DataJoint Relational Model","text":"<p>Manuscript:</p>  <p>Yatsenko D, Walker EY, Tolias AS. DataJoint: a simpler relational data model. arXiv:1807.11104. 2018 Jul 29. doi: https://doi.org/10.48550/arXiv.1807.11104</p>  <p>RRID:</p>  <p>DataJoint (RRID:SCR_014543)</p>","location":"citation/#datajoint-relational-model"},{"title":"DataJoint Elements","text":"<p>Manuscript:</p>  <p>Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358</p>  <p>RRID:</p>  <p>DataJoint Elements (RRID:SCR_021894) - Element <code>Enter Element name</code> (version <code>&lt;Enter version number&gt;</code>)</p>","location":"citation/#datajoint-elements"},{"title":"Projects","text":"","location":"projects/"},{"title":"Projects","text":"<p>DataJoint was originally developed by working systems neuroscientists at Baylor College of Medicine to meet the needs of their own research. Below is a partial list of known teams who use DataJoint.</p>","location":"projects/#projects"},{"title":"Multi-lab collaboratives","text":"<ul> <li>International Brain Lab (GitHub)</li> <li>Mesoscale Activity Project</li> <li>MICrONS</li> <li>Sainsbury Wellcome Centre Aeon</li> <li>U19 Projects<ul> <li>NYU Osmonauts</li> <li>Harvard DOPE</li> <li>Columbia MoC3</li> <li>Princeton BRAIN CoGS (GitHub)</li> <li>Rochester-NYU-Harvard Neural basis of causal inference</li> </ul> </li> </ul>","location":"projects/#multi-lab-collaboratives"},{"title":"Individual Labs and Researchers","text":"<ul> <li>Allen Institute<ul> <li>Mindscope Program</li> <li>Karel Svoboda Lab</li> <li>Forrest Collman</li> </ul> </li> <li>Arizona State University<ul> <li>Rick Gerkin Lab</li> </ul> </li> <li>Baylor College of Medicine<ul> <li>Nuo Li Lab</li> <li>Matthew McGinley Lab</li> <li>Paul Pfaffinger Lab</li> <li>Jacob Reimer Lab</li> <li>Andreas Tolias Lab</li> </ul> </li> <li>Boston University<ul> <li>Jerry Chen Lab</li> <li>Benjamin Scott Lab</li> </ul> </li> <li>California Institute of Technology<ul> <li>Roukes Group</li> <li>Siapas Lab</li> </ul> </li> <li>Cold Spring Harbor Laboratory<ul> <li>Engel Lab</li> </ul> </li> <li>Columbia University's Zuckerman Institute<ul> <li>Mark Churchland Lab</li> <li>Elizabeth Hillman Lab</li> <li>Rui Costa Lab</li> </ul> </li> <li>EPFL<ul> <li>Mackenzie Mathis Lab</li> </ul> </li> <li>FORTH<ul> <li>Emmanouil Froudarakis Lab (GitHub)</li> </ul> </li> <li>Harvard Medical School<ul> <li>Jan Drugowitsch Lab</li> <li>Harvey Lab</li> <li>Sabatini Lab</li> <li>Stelios Smirnakis Lab</li> </ul> </li> <li>Indiana University<ul> <li>Lu Lab</li> </ul> </li> <li>Johns Hopkins University<ul> <li>Applied Physics Lab (GitHub)</li> </ul> </li> <li>Kavli Institute for Systems Neuroscience<ul> <li>Moser Group</li> </ul> </li> <li>Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen<ul> <li>Busse Lab</li> <li>Katzner Lab</li> </ul> </li> <li>MIT<ul> <li>Fan Wang Lab</li> </ul> </li> <li>National Institute of Environmental Health Sciences<ul> <li>Eric E. Thomson</li> </ul> </li> <li>New York University<ul> <li>Dora Angelaki Lab</li> </ul> </li> <li>New York University Langone Medical Center<ul> <li>Tanya Sippy Lab</li> </ul> </li> <li>Northwestern University<ul> <li>James Cotton Lab (GitHub)</li> <li>Gregory Schwartz Lab (GitHub)</li> <li>Lucas Pinto Lab</li> </ul> </li> <li>Princeton University<ul> <li>Carlos Brody Lab</li> <li>David Tank Lab</li> <li>Ilana Witten Lab</li> <li>Jonathan Pillow Lab</li> <li>Seung Lab</li> </ul> </li> <li>Sainsbury Wellcome Centre<ul> <li>Tiago Branco Lab (GitHub)</li> </ul> </li> <li>Stanford University<ul> <li>Karl Deisseroth Lab</li> <li>Shaul Druckmann Lab</li> </ul> </li> <li>Tel-Aviv University<ul> <li>Arseny Finkelstein Lab (GitHub)</li> <li>Pablo Blinder Lab (GitHub)</li> </ul> </li> <li>University of Bonn<ul> <li>Tobias Rose Lab</li> </ul> </li> <li>University of California, Los Angeles<ul> <li>Anne Churchland Lab</li> </ul> </li> <li>University of California, San Diego<ul> <li>David Kleinfeld Lab (GitHub)</li> </ul> </li> <li>University of California, San Francisco<ul> <li>Loren Frank Lab</li> </ul> </li> <li>University of Oregon<ul> <li>Santiago Jaramillo Lab (GitHub)</li> <li>Michael Wehr Lab (GitHub)</li> </ul> </li> <li>University of Pennsylvania School of Medicine<ul> <li>John A. Dani</li> </ul> </li> <li>University of Rochester<ul> <li>Greg DeAngelis Lab</li> <li>Ralf Haefner Lab</li> </ul> </li> <li>Universit\u00e4t T\u00fcbingen<ul> <li>Berens Lab</li> <li>Euler Lab</li> <li>Macke Lab (GitHub)</li> </ul> </li> <li>University of Utah<ul> <li>Aleksandr Shcheglovitov Lab (GitHub)</li> </ul> </li> <li>University of Valencia<ul> <li>Kai-Hendrik Cohrs</li> </ul> </li> <li>University of Washington<ul> <li>Edgar Y. Walker Lab</li> <li>Tuthill Lab</li> </ul> </li> <li>Wilhelm Schickard Institute for Computer Science<ul> <li>Sinz Lab</li> <li>Euler Lab</li> <li>Bethge Lab</li> </ul> </li> <li>... and more labs</li> </ul>","location":"projects/#individual-labs-and-researchers"},{"title":"Publications","text":"<p>The following publications relied on DataJoint open-source software for data analysis.  If your work uses DataJoint or DataJoint Elements, please cite the respective manuscripts and RRIDs.</p>","location":"publications/"},{"title":"2022","text":"<ul> <li>Wang, Y., Chiola, S., Yang, G., Russell, C., Armstrong, C. J., Wu, Y., ... &amp; Shcheglovitov, A. (2022). Modeling human telencephalic development and autism-associated SHANK3 deficiency using organoids generated from single neural rosettes. Nature Communications, 13(1), 1-25.</li> </ul> <ul> <li>Franke, K., Willeke, K. F., Ponder, K., Galdamez, M., Zhou, N., Muhammad, T., ... &amp; Tolias, A. S. (2022). State-dependent pupil dilation rapidly shifts visual feature selectivity. Nature, 1-7.</li> </ul> <ul> <li>Pettit, N. H., Yap, E., Greenberg, M. E., Harvey, C. D. (2022). Fos ensembles encode and shape stable spatial maps in the hippocampus. Nature.</li> </ul> <ul> <li>Saunders, J. L., Ott, L. A., Wehr, M. (2022). AUTOPILOT: Automating experiments with lots of Raspberry Pis. bioRxiv.</li> </ul> <ul> <li>Born, G. (2022). The effect of feedback on sensory processing in the mouse early visual system. Doctoral dissertation.</li> </ul> <ul> <li>Cadena, S. A., Willeke, K. F., Restivo, K., Denfield, G., Sinz, F. H., Bethge, M., ... &amp; Ecker, A. S. (2022). Diverse task-driven modeling of macaque V4 reveals functional specialization towards semantic tasks.. bioRxiv.</li> </ul> <ul> <li>Cotton, R. J. (2022). PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research. arXiv. 2203.08792.</li> </ul> <ul> <li>Cotton, R. J., McClerklin, E., Cimorelli, A., &amp; Patel, A. (2022). Spatiotemporal characterization of gait from monocular videos with transformers.</li> </ul> <ul> <li>Cotton, R. J., McClerklin, E., Cimorelli, A., Patel, A., &amp; Karakostas, T. (2022). Transforming Gait: Video-Based Spatiotemporal Gait Analysis. arXiv. 2203.09371.</li> </ul> <ul> <li>Fu, J., Willeke, K. F., Pierzchlewicz, P. A., Muhammad, T., Denfield, G. H., Sinz, F. H., &amp; Tolias, A. S. (2022). Heterogeneous Orientation Tuning Across Sub-Regions of Receptive Fields of V1 Neurons in Mice. Available at SSRN 4029075.</li> </ul> <ul> <li>Goetz, J., Jessen, Z. F., Jacobi, A., Mani, A., Cooler, S., Greer, D., ... &amp; Schwartz, G. W. (2022). Unified classification of mouse retinal ganglion cells using function, morphology, and gene expression. Cell reports, 40(2), 111040.</li> </ul> <ul> <li>Jaffe, A. (2022). Optical investigation of microcircuit computations in mouse primary visual cortex. Doctoral dissertation.</li> </ul> <ul> <li>Obenhaus, H.A., Zong, W., Jacobsen, R.I., Rose, T., Donato, F., Chen, L., Cheng, H., Bonhoeffer, T., Moser, M.B. &amp; Moser, E.I. (2022). Functional network topography of the medial entorhinal cortex. Proceedings of the National Academy of Sciences, 119 (7).</li> </ul> <ul> <li>Roukes, M. L. (2022, May). The Integrated Neurophotonics Paradigm. In CLEO: Applications and Technology (pp. ATh4I-6). Optica Publishing Group.</li> </ul> <ul> <li>Sanchez, M., Moore, D., Johnson, E. C., Wester, B., Lichtman, J. W., &amp; Gray-Roncal, W. (2022). Connectomics Annotation Metadata Standardization for Increased Accessibility and Queryability. Frontiers in Neuroinformatics.</li> </ul> <ul> <li>Spacek, M. A., Crombie, D., Bauer, Y., Born, G., Liu, X., Katzner, S., &amp; Busse, L. (2022). Robust effects of corticothalamic feedback and behavioral state on movie responses in mouse dLGN. Elife, 11, e70469.</li> </ul> <ul> <li>Tseng, S. Y., Chettih, S. N., Arlt, C., Barroso-Luque, R., &amp; Harvey, C. D. (2022). Shared and specialized coding across posterior cortical areas for dynamic navigation decisions.. Neuron.</li> </ul> <ul> <li>Turner, N. L., Macrina, T., Bae, J. A., Yang, R., Wilson, A. M., Schneider-Mizell, C., ... &amp; Seung, H. S. (2022). Reconstruction of neocortex: Organelles, compartments, cells, circuits, and activity. . Cell, 185(6), 1082-1100.</li> </ul> <ul> <li>Ustyuzhaninov, I., Burg, M.F., Cadena, S.A., Fu, J., Muhammad, T., Ponder, K., Froudarakis, E., Ding, Z., Bethge, M., Tolias, A. &amp; Ecker, A.S. (2022). Digital twin reveals combinatorial code of non-linear computations in the mouse primary visual cortex. bioRxiv.</li> </ul> <ul> <li>Willeke, K. F., Fahey, P. G., Bashiri, M., Pede, L., Burg, M. F., Blessing, C., ... &amp; Sinz, F. H. (2022). The Sensorium competition on predicting large-scale mouse primary visual cortex activity. arXiv preprint arXiv:2206.08666.</li> </ul>","location":"publications/#2022"},{"title":"2021","text":"<ul> <li>Bae, J. A., Baptiste, M., Bodor, A. L., Brittain, D., Buchanan, J., Bumbarger, D. J., Castro, M. A., Celii, B., Cobos, E., Collman, F., ... (2021). Functional connectomics spanning multiple areas of mouse visual cortex. bioRxiv.</li> </ul> <ul> <li>Born, G., Schneider-Soupiadis, F. A., Erisken, S., Vaiceliunaite, A., Lao, C. L., Mobarhan, M. H., Spacek, M. A., Einevoll, G. T., &amp; Busse, L. (2021). Corticothalamic feedback sculpts visual spatial integration in mouse thalamus. Nature Neuroscience, 24(12), 1711\u20131720.</li> </ul> <ul> <li>Burg, M. F., Cadena, S. A., Denfield, G. H., Walker, E. Y., Tolias, A. S., Bethge, M., &amp; Ecker, A. S. (2021). Learning divisive normalization in primary visual cortex. PLOS Computational Biology, 17(6), e1009028.</li> </ul> <ul> <li>Claudi, F., Campagner, D., &amp; Branco, T. (2021). Innate heuristics and fast learning support escape route selection in mice. bioRxiv.</li> </ul> <ul> <li>Cohrs, K.H. (2021). Investigation of feedback mechanisms in visual cortex using deep learning models [Master\u2019s thesis]. University of G\u00f6ttingen.</li> </ul> <ul> <li>Finkelstein, A., Fontolan, L., Economo, M. N., Li, N., Romani, S., &amp; Svoboda, K. (2021). Attractor dynamics gate cortical information flow during decision-making. Nature Neuroscience. 24(6), 843-850.</li> </ul> <ul> <li>Franke, K., Willeke, K. F., Ponder, K., Galdamez, M., Muhammad, T., Patel, S., Froudarakis, E., Reimer, J., Sinz, F., &amp; Tolias, A. (2021). Behavioral state tunes mouse vision to ethological features through pupil dilation. bioRxiv.</li> </ul> <ul> <li>Jacobsen, R. I., Nair, R. R., Obenhaus, H. A., Donato, F., Slettmoen, T., Moser, M.-B., &amp; Moser, E. I. (2021). All-viral tracing of monosynaptic inputs to single birthdate-defined neurons in the intact brain. bioRxiv.</li> </ul> <ul> <li>Laboratory, T. I. B., Aguillon-Rodriguez, V., Angelaki, D., Bayer, H., Bonacchi, N., Carandini, M., Cazettes, F., Chapuis, G., Churchland, A. K., Dan, Y., ... (2021). Standardized and reproducible measurement of decision-making in mice. eLife, 10.</li> </ul> <ul> <li>Strauss, S., Korympidou, M. M., Ran, Y., Franke, K., Schubert, T., Baden, T., Berens, P., Euler, T., &amp; Vlasits, A. L. (2021). Center-surround interactions underlie bipolar cell motion sensing in the mouse retina. bioRxiv.</li> </ul> <ul> <li>Subramaniyan, M., Manivannan, S., Chelur, V., Tsetsenis, T., Jiang, E., &amp; Dani, J. A. (2021). Fear conditioning potentiates the hippocampal CA1 commissural pathway in vivo and increases awake phase sleep. Hippocampus, 31(10), 1154\u20131175.</li> </ul> <ul> <li>Urai, A. E., Aguillon-Rodriguez, V., Laranjeira, I. C., Cazettes, F., Laboratory, T. I. B., Mainen, Z. F., &amp; Churchland, A. K. (2021). Citric acid water as an alternative to water restriction for high-yield mouse behavior. Eneuro, 8(1).</li> </ul> <ul> <li>Wal, A., Klein, F. J., Born, G., Busse, L., &amp; Katzner, S. (2021). Evaluating visual cues modulates their representation in mouse visual and cingulate cortex. Journal of Neuroscience, 41(15), 3531\u20133544.</li> </ul> <ul> <li>Wang, Y., Chiola, S., Yang, G., Russell, C., Armstrong, C. J., Wu, Y., Spampanato, J., Tarboton, P., Chang, A. N., Harmin, D. A., ... (2021). Modeling autism-associated SHANK3 deficiency using human cortico-striatal organoids generated from single neural rosettes. bioRxiv.</li> </ul>","location":"publications/#2021"},{"title":"2020","text":"<ul> <li>Aguillon-Rodriguez, V., Angelaki, D. E., Bayer, H. M., Bonacchi, N., Carandini, M., Cazettes, F., Chapuis, G. A., Churchland, A. K., Dan, Y., Dewitt, E. E., ... (2020). A standardized and reproducible method to measure decision-making in mice. BioRxiv.</li> </ul> <ul> <li>Angelaki, D. E., Ng, J., Abrego, A. M., Cham, H. X., Asprodini, E. K., Dickman, J. D., &amp; Laurens, J. (2020). A gravity-based three-dimensional compass in the mouse brain. Nature Communications, 11(1), 1\u201313.</li> </ul> <ul> <li>Cotton, R. J., Sinz, F. H., &amp; Tolias, A. S. (2020). Factorized neural processes for neural processes: K-shot prediction of neural responses. arXiv Preprint arXiv:2010.11810.</li> </ul> <ul> <li>Heath, S. L., Christenson, M. P., Oriol, E., Saavedra-Weisenhaus, M., Kohn, J. R., &amp; Behnia, R. (2020). Circuit mechanisms underlying chromatic encoding in drosophila photoreceptors. Current Biology.</li> </ul> <ul> <li>Laturnus, S., Kobak, D., &amp; Berens, P. (2020). A systematic evaluation of interneuron morphology representations for cell type discrimination. Neuroinformatics, 18(4), 591\u2013609.</li> </ul> <ul> <li>Sinz, F. H., Sachgau, C., Henninger, J., Benda, J., &amp; Grewe, J. (2020). Simultaneous spike-time locking to multiple frequencies. Journal of Neurophysiology, 123(6), 2355\u20132372.</li> </ul> <ul> <li>Yatsenko, D., Moreaux, L. C., Choi, J., Tolias, A., Shepard, K. L., &amp; Roukes, M. L. (2020). Signal separability in integrated neurophotonics. bioRxiv.</li> </ul> <ul> <li>Zhao, Z., Klindt, D. A., Chagas, A. M., Szatko, K. P., Rogerson, L., Protti, D. A., Behrens, C., Dalkara, D., Schubert, T., Bethge, M., others. (2020). The temporal structure of the inner retina at a single glance. Scientific Reports, 10(1), 1\u201317.</li> </ul>","location":"publications/#2020"},{"title":"2019","text":"<ul> <li>Bonacchi, N., Chapuis, G., Churchland, A., Harris, K. D., Rossant, C., Sasaki, M., Shen, S., Steinmetz, N. A., Walker, E. Y., Winter, O., others. (2019). Data architecture and visualization for a large-scale neuroscience collaboration. BioRxiv, 827873.</li> </ul> <ul> <li>Cadena, S. A., Denfield, G. H., Walker, E. Y., Gatys, L. A., Tolias, A. S., Bethge, M., &amp; Ecker, A. S. (2019). Deep convolutional models improve predictions of macaque V1 responses to natural images. PLoS Computational Biology, 15(4), e1006897.</li> </ul> <ul> <li>Chettih, S. N., &amp; Harvey, C. D. (2019). Single-neuron perturbations reveal feature-specific competition in V1. Nature, 567(7748), 334\u2013340.</li> </ul> <ul> <li>Fahey, P. G., Muhammad, T., Smith, C., Froudarakis, E., Cobos, E., Fu, J., Walker, E. Y., Yatsenko, D., Sinz, F. H., Reimer, J., ... (2019). A global map of orientation tuning in mouse visual cortex. bioRxiv, 745323.</li> </ul> <ul> <li>Laurens, J., Abrego, A., Cham, H., Popeney, B., Yu, Y., Rotem, N., Aarse, J., Asprodini, E. K., Dickman, J. D., &amp; Angelaki, D. E. (2019). Multiplexed code of navigation variables in anterior limbic areas. bioRxiv, 684464.</li> </ul> <ul> <li>Liu, G., Froudarakis, E., Patel, J. M., Kochukov, M. Y., Pekarek, B., Hunt, P. J., Patel, M., Ung, K., Fu, C.-H., Jo, J., ... (2019). Target specific functions of EPL interneurons in olfactory circuits. Nature Communications, 10(1), 1\u201314.</li> </ul> <ul> <li>Ros\u00f3n, M. R., Bauer, Y., Kotkat, A. H., Berens, P., Euler, T., &amp; Busse, L. (2019). Mouse dLGN receives functional input from a diverse population of retinal ganglion cells with limited convergence. Neuron, 102(2), 462\u2013476.</li> </ul> <ul> <li>Walker, E. Y., Sinz, F. H., Cobos, E., Muhammad, T., Froudarakis, E., Fahey, P. G., Ecker, A. S., Reimer, J., Pitkow, X., &amp; Tolias, A. S. (2019). Inception loops discover what excites neurons most using deep predictive models. Nature Neuroscience, 22(12), 2060\u20132065.</li> </ul>","location":"publications/#2019"},{"title":"2018","text":"<ul> <li>Denfield, G. H., Ecker, A. S., Shinn, T. J., Bethge, M., &amp; Tolias, A. S. (2018). Attentional fluctuations induce shared variability in macaque primary visual cortex. Nature Communications, 9(1), 2654.</li> </ul> <ul> <li>Ecker, A. S., Sinz, F. H., Froudarakis, E., Fahey, P. G., Cadena, S. A., Walker, E. Y., Cobos, E., Reimer, J., Tolias, A. S., &amp; Bethge, M. (2018). A rotation-equivariant convolutional neural network model of primary visual cortex. arXiv Preprint arXiv:1809.10504.</li> </ul> <ul> <li>Sinz, F., Ecker, A. S., Fahey, P., Walker, E., Cobos, E., Froudarakis, E., Yatsenko, D., Pitkow, Z., Reimer, J., &amp; Tolias, A. (2018). Stimulus domain transfer in recurrent models for large scale cortical population prediction on video. Advances in Neural Information Processing Systems, 7199\u20137210.</li> </ul> <ul> <li>Walker, E. Y., Sinz, F. H., Froudarakis, E., Fahey, P. G., Muhammad, T., Ecker, A. S., Cobos, E., Reimer, J., Pitkow, X., &amp; Tolias, A. S. (2018). Inception in visual cortex: In vivo-silico loops reveal most exciting images. bioRxiv, 506956.</li> </ul>","location":"publications/#2018"},{"title":"2017","text":"<ul> <li>Franke, K., Berens, P., Schubert, T., Bethge, M., Euler, T., &amp; Baden, T. (2017). Inhibition decorrelates visual feature representations in the inner retina. Nature, 542(7642), 439.</li> </ul> <ul> <li>Jurjut, O., Georgieva, P., Busse, L., &amp; Katzner, S. (2017). Learning enhances sensory processing in mouse V1 before improving behavior. Journal of Neuroscience, 37(27), 6460\u20136474.</li> </ul> <ul> <li>Shan, K. Q., Lubenov, E. V., &amp; Siapas, A. G. (2017). Model-based spike sorting with a mixture of drifting t-distributions. Journal of Neuroscience Methods, 288, 82\u201398.</li> </ul>","location":"publications/#2017"},{"title":"2016","text":"<ul> <li>Baden, T., Berens, P., Franke, K., Ros\u00f3n, M. R., Bethge, M., &amp; Euler, T. (2016). The functional diversity of retinal ganglion cells in the mouse. Nature, 529(7586), 345\u2013350.</li> </ul> <ul> <li>Cadwell, C. R., Palasantza, A., Jiang, X., Berens, P., Deng, Q., Yilmaz, M., Reimer, J., Shen, S., Bethge, M., Tolias, K. F., others. (2016). Electrophysiological, transcriptomic and morphologic profiling of single neurons using patch-seq. Nature Biotechnology, 34(2), 199\u2013203.</li> </ul> <ul> <li>Hartmann, L., Drewe-Bo\u00df, P., Wie\u00dfner, T., Wagner, G., Geue, S., Lee, H.-C., Oberm\u00fcller, D. M., Kahles, A., Behr, J., Sinz, F. H., ... (2016). Alternative splicing substantially diversifies the transcriptome during early photomorphogenesis and correlates with the energy availability in arabidopsis. The Plant Cell, 28(11), 2715\u20132734.</li> </ul> <ul> <li>Khastkhodaei, Z., Jurjut, O., Katzner, S., &amp; Busse, L. (2016). Mice can use second-order, contrast-modulated stimuli to guide visual perception. Journal of Neuroscience, 36(16), 4457\u20134469.</li> </ul> <ul> <li>Reimer, J., McGinley, M. J., Liu, Y., Rodenkirch, C., Wang, Q., McCormick, D. A., &amp; Tolias, A. S. (2016). Pupil fluctuations track rapid changes in adrenergic and cholinergic activity in cortex. Nature Communications, 7, 13289.</li> </ul> <ul> <li>Shan, K. Q., Lubenov, E. V., Papadopoulou, M., &amp; Siapas, A. G. (2016). Spatial tuning and brain state account for dorsal hippocampal CA1 activity in a non-spatial learning task. eLife, 5, e14321.</li> </ul>","location":"publications/#2016"},{"title":"2015","text":"<ul> <li>Jiang, X., Shen, S., Cadwell, C. R., Berens, P., Sinz, F., Ecker, A. S., Patel, S., &amp; Tolias, A. S. (2015). Principles of connectivity among morphologically defined cell types in adult neocortex. Science, 350(6264), aac9462.</li> </ul> <ul> <li>Yatsenko, D., Josi\u0107, K., Ecker, A. S., Froudarakis, E., Cotton, R. J., &amp; Tolias, A. S. (2015). Improved estimation and interpretation of correlations in neural circuits. PLoS Comput Biol, 11(3), e1004083.</li> </ul>","location":"publications/#2015"},{"title":"2014","text":"<ul> <li>Ecker, A. S., Berens, P., Cotton, R. J., Subramaniyan, M., Denfield, G. H., Cadwell, C. R., Smirnakis, S. M., Bethge, M., &amp; Tolias, A. S. (2014). State dependence of noise correlations in macaque primary visual cortex. Neuron, 82(1), 235\u2013248.</li> </ul> <ul> <li>Erisken, S., Vaiceliunaite, A., Jurjut, O., Fiorini, M., Katzner, S., &amp; Busse, L. (2014). Effects of locomotion extend throughout the mouse early visual system. Current Biology, 24(24), 2899\u20132907.</li> </ul> <ul> <li>Froudarakis, E., Berens, P., Ecker, A. S., Cotton, R. J., Sinz, F. H., Yatsenko, D., Saggau, P., Bethge, M., &amp; Tolias, A. S. (2014). Population code in mouse V1 facilitates readout of natural scenes through increased sparseness. Nat Neurosci, 17(6), 851\u2013857.</li> </ul> <ul> <li>Reimer, J., Froudarakis, E., Cadwell, C. R., Yatsenko, D., Denfield, G. H., &amp; Tolias, A. S. (2014). Pupil fluctuations track fast switching of cortical states during quiet wakefulness. Neuron, 84(2), 355\u2013362.</li> </ul>","location":"publications/#2014"},{"title":"2013","text":"<ul> <li>Cotton, R. J., Froudarakis, E., Storer, P., Saggau, P., &amp; Tolias, A. S. (2013). Three-dimensional mapping of microcircuit correlation structure. Frontiers in Neural Circuits, 7, 151.</li> </ul> <ul> <li>Vaiceliunaite, A., Erisken, S., Franzen, F., Katzner, S., &amp; Busse, L. (2013). Spatial integration in mouse primary visual cortex. Journal of Neurophysiology, 110(4), 964\u2013972.</li> </ul>","location":"publications/#2013"},{"title":"Contribution Guidelines","text":"<p>Thank you for your interest in contributing! \ud83e\udd1d</p> <p>To help keep everyone in alignment and coordinated in the community effort, we\u2019ve created this document. It serves as the contribution guideline that outlines how open-source software development is to be conducted. Any software development that makes reference to this document can be assumed to adopt the policies outlined below. We\u2019ve structured the guideline in a FAQ (frequently asked questions) format to make it easier to digest. Feel free to review the questions below to determine any specific policy.</p> <p>The principal maintainer of DataJoint and associated tools is the DataJoint company. The pronouns \u201cwe\u201d and \u201cus\u201d in this guideline refer to the principal maintainers. We invite reviews and contributions of the open-source software. We compiled these guidelines to make this work clear and efficient.</p>","location":"community/contribute/"},{"title":"Feedback","text":"<p>DataJoint APIs, DataJoint Web GUIs, and DataJoint Elements are supported by NIH grant U24 NS116470 for disseminating open-source software for neuroscience research. Your feedback is essential for continued funding. Your feedback also helps shape the technology development roadmap for the DataJoint ecosystem. Please tell us about your projects by filling out the DataJoint Census.</p>","location":"community/contribute/#feedback"},{"title":"1) Which issue should I contribute towards?","text":"<p>There are three primary things to consider when looking to contribute.</p> <p>Availability: An indication of whether anyone is currently working on a fix for the given issue. Availability is indicated by who is <code>assigned</code>. Issues that are <code>unassigned</code> mean that there is no one yet working on resolving the issue and the issue is available for someone to work on. If an issue has been assigned, then any additional work on that issue should be coordinated with the assignee.</p> <p>Specification: In order for issues to be properly addressed, the requirements of satisfying and closing the issue should be clear. If it is not, a label will be added as <code>unspecified</code>. This could be due to more debug info being necessary, more details on intended behavior, or perhaps that further discussion is required to determine a good solution. Feel free to help us arrive at a proper specification.</p> <p>Priority: As a community, we work on a concerted effort to bring about the realization of the milestones. We utilize milestones as a planning tool to help focus a group of changes around a release. To determine the priority of issues, simply have a look at the next milestone that is expected to arrive. Therefore, each milestone following this can be understood as lower in priority respectively. Bear in mind that much like a hurricane forecast, the execution plan is much more likely to be accurate the closer to today\u2019s date as opposed to milestones further out. Extremely low priority issues are assigned to the <code>Backburner</code> milestone. Since <code>Backburner</code> does not have a target date, this indicates that its issues may be deferred indefinitely. Occasionally the maintainers will move issues from <code>Backburner</code> as it makes sense to address them within a release. Also, issues <code>unassigned</code> to a milestone can be understood as new issues which have not been triaged.</p> <p>After considering the above, you may comment on the issue you\u2019d like to help fix and a maintainer will assign it to you.</p>","location":"community/contribute/#1-which-issue-should-i-contribute-towards"},{"title":"2) What is the proper etiquette for proposing changes as contribution?","text":"<p>What is generally expected from new contributions are the following:</p> <p>Any proposed contributor changes should be introduced in the form of a pull request (PR) from their fork.</p> <p>Proper branch target specified. The following are the generally the available branches that can be targeted:</p> <ul> <li><code>main</code> or <code>master</code>: Represents the single source of truth and the latest in completed development.</li> </ul> <ul> <li><code>pre</code>: Represents the source at the point of the last stable release.</li> </ul> <p>For larger more involved changes, a maintainer may determine it best to create a feature-specific branch and adjust the PR accordingly.</p> <p>A summary description that describes the overall intent behind the PR.</p> <p>Proper links to the issue(s) that the PR serves to resolve.</p> <p>Newly introduced changes must pass any required checks. Typically as it relates to tests, this means:</p> <ol> <li>No syntax errors</li> <li>No integration errors</li> <li>No style errors e.g. PEP8, etc.</li> <li>Similar or better code coverage</li> </ol> <p>Additional documentation to reflect new feature or behavior introduced.</p> <p>Necessary updates to the changelog following Keep a Changelog convention.</p> <p>A contributor should not approve or merge their own PR.</p> <p>Reviewer suggestions or feedback should not be directly committed to a branch on a contributor\u2019s fork. A less intrusive way to collaborate would be for the reviewer to PR to the contributor\u2019s fork/branch that is associated with the main PR currently in review.</p> <p>Maintainers will also ensure that PR\u2019s have the appropriate assignment for reviewer, milestone, and project.</p>","location":"community/contribute/#2-what-is-the-proper-etiquette-for-proposing-changes-as-contribution"},{"title":"3) How can I track the progress of an issue that has been assigned?","text":"<p>Since milestones represent the development plan, projects represent the actual execution. Projects are typically fixed-time sprints (1-2 weeks). A \u2018workable\u2019 number of issues that have been assigned to developers and assigned to the next milestone are selected and tracked in each project to provide greater granularity in the week-to-week progress. Automation is included observing the <code>Automated kanban with reviews</code> template. Maintainers will adjust the project assignment to reflect the order in which to resolve the milestone issues.</p>","location":"community/contribute/#3-how-can-i-track-the-progress-of-an-issue-that-has-been-assigned"},{"title":"4) What is the release process? How do I know when my merged contribution will officially make it into a release?","text":"<p>Releases follow the standard definition of semantic versioning. Meaning:</p> <p><code>MAJOR</code> . <code>MINOR</code> . <code>PATCH</code></p> <ul> <li><code>MAJOR</code> version when you make incompatible API changes,</li> </ul> <ul> <li><code>MINOR</code> version when you add functionality in a backwards compatible manner, and</li> </ul> <ul> <li><code>PATCH</code> version when you make backwards compatible bug fixes.</li> </ul> <p>Each release requires tagging the commit appropriately and is then issued in the normal medium for release e.g. PyPi, NPM, YARN, GitHub Release, etc.</p> <p>Minor releases are triggered when all the issues assigned to a milestone are resolved and closed. Patch releases are triggered periodically from <code>main</code> or <code>master</code> after a reasonable number of PR merges have come in.</p>","location":"community/contribute/#4-what-is-the-release-process-how-do-i-know-when-my-merged-contribution-will-officially-make-it-into-a-release"},{"title":"5) I am not yet too comfortable contributing but would like to engage the community. What is the policy on community engagement?","text":"<p>In order to follow the appropriate process and setting, please reference the following flow for your desired mode of engagement:</p>","location":"community/contribute/#5-i-am-not-yet-too-comfortable-contributing-but-would-like-to-engage-the-community-what-is-the-policy-on-community-engagement"},{"title":"5a) Generally, how do I perform ____?","text":"<p>If the documentation does not provide clear enough instruction, please see StackOverflow posts related to the datajoint tag or ask a new question tagging it appropriately. You may refer to our  datajoint tag wiki for more details on its proper use.</p>","location":"community/contribute/#5a-generally-how-do-i-perform-____"},{"title":"5b) I just encountered this error, how can I resolve it?","text":"<p>Please see StackOverflow posts related to the  datajoint tag or ask a new  question tagging it appropriately. You may refer to our  datajoint tag wiki for more details on its proper use.</p>","location":"community/contribute/#5b-i-just-encountered-this-error-how-can-i-resolve-it"},{"title":"5c) I just encountered this error and I am sure it is a bug, how do I report it?","text":"<p>Please file it under the issue tracker associated with the open-source software.</p>","location":"community/contribute/#5c-i-just-encountered-this-error-and-i-am-sure-it-is-a-bug-how-do-i-report-it"},{"title":"5d) I have an idea or new feature request, how do I submit it?","text":"<p>Please file it under the issue tracker associated with the open-source software.</p>","location":"community/contribute/#5d-i-have-an-idea-or-new-feature-request-how-do-i-submit-it"},{"title":"5e) I am curious why the maintainers choose to ____? i.e. questions that are \u2018opinionated\u2019 in nature with answers that some might disagree.","text":"<p>Please join the community on the  DataJoint Slack and ask on the most relevant channel. There, you may engage directly with the maintainers for proper discourse.</p>","location":"community/contribute/#5e-i-am-curious-why-the-maintainers-choose-to-____-ie-questions-that-are-opinionated-in-nature-with-answers-that-some-might-disagree"},{"title":"5f) What is the timeline or roadmap for the release of certain supported features?","text":"<p>Please refer to milestones and projects associated with the open-source software.</p>","location":"community/contribute/#5f-what-is-the-timeline-or-roadmap-for-the-release-of-certain-supported-features"},{"title":"5g) I need urgent help best suited for live debugging, how can I reach out directly?","text":"<p>Please join the community on the  DataJoint Slack and ask on the most relevant channel. Please bear in mind that as open-source community software, availability of the maintainers might be limited.</p>","location":"community/contribute/#5g-i-need-urgent-help-best-suited-for-live-debugging-how-can-i-reach-out-directly"},{"title":"Events","text":"<p>Find us at the following workshops and conferences!</p>","location":"community/events/"},{"title":"DataJoint Office Hours","text":"<p>The DataJoint open source team are offering monthly Office Hours (see details here). Sign up here!</p>","location":"community/events/#datajoint-office-hours"},{"title":"Upcoming Workshops","text":"<ul> <li>Society for Neuroscience: November 12-16, 2022</li> </ul>","location":"community/events/#upcoming-workshops"},{"title":"Past Events","text":"<ul> <li>Senses in Motion Symposium: October 17, 2022</li> </ul> <ul> <li>NeuroDataReHack Hackathon: October 3-5, 2022</li> </ul> <ul> <li>Neuropixels and OpenScope Workshop: September 21-23, 2022</li> </ul> <ul> <li>INCF Assembly: September 12-16, 2022</li> </ul> <ul> <li>Research Workflows Workshop: September 6-8, 2022</li> </ul> <ul> <li>NWB Hackathon User Days: July 24-27, 2022</li> </ul> <ul> <li>NIH BRAIN Initiative Meeting: June 21-22, 2022</li> </ul> <ul> <li>DataJoint Office Hours: May 20, 2022<ul> <li>MATLAB/Python interoperability. Video Link</li> </ul> </li> </ul> <ul> <li>DataJoint Office Hours: April 27, 2022<ul> <li>Filepath handling, <code>linking_module</code>, and <code>key</code> management in <code>make</code> functions.</li> </ul> </li> </ul> <ul> <li>UCL Neuropixels Course: October 19, 2021</li> </ul> <ul> <li>INCF Neuroinformatics Training Week: August 30 - September 2, 2021<ul> <li>Session 1 Recording</li> <li>Session 2 Recording</li> <li>Session 3 Recording</li> <li>Session 4 Recording</li> </ul> </li> </ul> <ul> <li>NYU \u2018FAIR Thee Well\u2019 Symposium: August 9-10, 2021<ul> <li>Sessions 1-3 Recordings</li> </ul> </li> </ul> <ul> <li>Neuromatch Academy: July 29, 2021<ul> <li>Session 1 Recording</li> <li>Session 2 Recording</li> </ul> </li> </ul>","location":"community/events/#past-events"},{"title":"History","text":"<p>Dimitri Yatsenko began development of DataJoint in Andreas S.Tolias' lab in the Neuroscience Department at Baylor College of Medicine in the fall of 2009. Initially implemented as a thin MySQL API in MATLAB, it defined the major principles of the DataJoint model.</p> <p>Many students and postdocs in the lab as well as collaborators and early adopters have contributed to the project. Jacob Reimer and Emmanouil Froudarakis became early adopters in Andreas Tolias' Lab and propelled development. Alexander S. Ecker, Philipp Berens, Andreas Hoenselaar, and R. James Cotton contributed to the formulation of the overall requirements for the data model and critical reviews of DataJoint development.</p> <p>Outside the Tolias lab, the first labs to adopt DataJoint (approx. 2010) were the labs of Athanassios G. Siapas at CalTech, Laura Busse and Steffen Katzner at the University of T\u00fcbingen.</p> <p>In 2015, the Python implementation gained momentum with Edgar Y. Walker and Fabian Sinz joining as principal contributors.</p> <p>In 2016, Andreas Tolias Lab joined the MICrONS project, using DataJoint to process volumes of neurophysiology and neuroanatomical data shared across large teams.</p> <p>In 2016, Vathes LLC was founded to provide support to groups using DataJoint.</p> <p>In 2017, DARPA awarded a small-business innovation research grant to Vathes LLC (Contract D17PC00162) to further develop and publicize the DataJoint framework.</p> <p>In June 2018, the Princeton Neuroscience Institute, under the leadership of Prof. Carlos Brody, began funding a project to generate a detailed DataJoint user manual.</p>","location":"community/history/"},{"title":"Licenses","text":"<p>The resources are distributed under the following licenses which are included in the respective repositories:</p>    Resource License     DataJoint API for Python LGPLv3   DataJoint API for MATLAB MIT   Pharus MIT   DataJoint LabBook MIT   DataJoint SciViz MIT   DataJoint Elements and Workflows MIT","location":"community/licenses/"},{"title":"Support","text":"<ul> <li>General how-to or error question? Post on StackOverflow with the <code>datajoint</code> tag</li> </ul> <ul> <li>Bug report or feature request? Submit a GitHub issue to the relevant repository</li> </ul> <ul> <li>For open-ended discussions, join us on the DataJoint Slack</li> </ul> <ul> <li> <p>DataJoint Office Hours</p> <ul> <li>The DataJoint open source team will be offering monthly Office Hours.  These are   public workshops on Zoom where we offer two half-hour sessions of free hands-on   support to teams using DataJoint software.</li> </ul> <ul> <li>For each half-hour session, a team can sign up to work through their specific   question.  Teams who wish to receive this support can either share their workflow   directly or work with DataJoint engineers to replicate the issue in a test   environment.   To be fair to the community, no team will be selected for support   more than once per quarter.</li> </ul> <ul> <li>Sign up for Office Hours!</li> </ul> </li> </ul> <p>For fully managed services, we invite you to visit  DataJoint for solutions or to chat with our team for more information.</p>","location":"community/support/"},{"title":"DANDI","text":"","location":"community/partnerships/dandi/"},{"title":"Sustainability Roadmap between DataJoint Elements and DANDI Archive","text":"<p> </p>","location":"community/partnerships/dandi/#sustainability-roadmap-between-datajoint-elements-and-dandi-archive"},{"title":"Aim","text":"<p>DataJoint Elements and The DANDI Archive (DANDI) are two neuroinformatics initiatives in active development. The projects develop independently yet they have complementary aims and overlapping user communities. This document establishes key processes for coordinating development and communications in order to promote integration and interoperability across the two ecosystems.</p>","location":"community/partnerships/dandi/#aim"},{"title":"Projects and Teams","text":"","location":"community/partnerships/dandi/#projects-and-teams"},{"title":"DataJoint","text":"<p>DataJoint Elements \u2014 https://datajoint.com/docs/elements/ \u2014 is a collection of   open-source reference database schemas and analysis workflows for neurophysiology   experiments, supported by DataJoint \u2014 https://datajoint.com/docs/core/ \u2014 an   open-source software framework. The project is funded by the NIH grant U24 NS116470   and led by Dr. Dimitri Yatsenko.</p> <p>The principal developer of DataJoint Elements and the DataJoint framework is the company DataJoint \u2014 https://datajoint.com.</p>","location":"community/partnerships/dandi/#datajoint"},{"title":"Neurodata without Borders (NWB)","text":"<p>DANDI - https://dandiarchive.org \u2014 is an archive for neurophysiology data,  providing neuroscientists with a common platform to share, archive, and process data.  The project is funded by the NIH grant R24 MH117295 and led by Dr. Satrajit S. Ghosh  and Dr. Yaroslav O. Halchenko.</p> <p>The principal developers of DANDI are at the Massachusetts Institute of Technology,  Dartmouth College, Catalyst Neuro, and Kitware.</p>","location":"community/partnerships/dandi/#neurodata-without-borders-nwb"},{"title":"General Principles","text":"","location":"community/partnerships/dandi/#general-principles"},{"title":"No obligation","text":"<p>The developers of the two ecosystems acknowledge that this roadmap document creates no  contractual relationship between them but they agree to work together in the spirit of  partnership to ensure that there is a united, visible, and responsive leadership and to  demonstrate administrative and managerial commitment to coordinate development and  communications.</p>","location":"community/partnerships/dandi/#no-obligation"},{"title":"Coordinated Development","text":"<p>The two projects will coordinate their development approaches to ensure maximum interoperability. This includes:</p> <ul> <li>coordinated use of terminology and nomenclatures</li> <li>support for testing infrastructure: unit testing and integration testing</li> <li>a coordinated software release process and versioning</li> <li>coordinated resolution of issues arising from joint use of the two tools</li> </ul>","location":"community/partnerships/dandi/#coordinated-development"},{"title":"Points of Contact","text":"<p>To achieve the aims of coordinated development, both projects appoint a primary point of contact (POC) to respond to questions relating to the integration and interoperability  of DataJoint Elements and DANDI.</p> <p>For 2022, the DataJoint Elements POC is Dr. Chris Brozdowski (cbroz@datajoint.com)</p> <p>For 2022, the DANDI POC is Dr.Satrajit Ghosh (satra@mit.edu)</p>","location":"community/partnerships/dandi/#points-of-contact"},{"title":"Annual Review","text":"<p>To achieve the aims of coordinated development, the principal developers conduct a  joint annual review of this roadmap document to ensure that the two programs are well  integrated and not redundant. The contents and resolutions of the review will be made  publicly available.</p>","location":"community/partnerships/dandi/#annual-review"},{"title":"Licensing","text":"<p>The two parties ensure that relevant software components are developed under licenses that avoid any hindrance to integration and interoperability between DataJoint Elements and DANDI.</p>","location":"community/partnerships/dandi/#licensing"},{"title":"Development Roadmap","text":"<ul> <li>Mechanism to upload to DANDI - Completed 2022 -    Element Interface DANDI module</li> </ul> <ul> <li>Documentation to upload to DANDI - Completed 2022 -    Jupyter notebook</li> </ul>","location":"community/partnerships/dandi/#development-roadmap"},{"title":"Facemap","text":"","location":"community/partnerships/facemap/"},{"title":"Sustainability Roadmap between DataJoint Elements and Facemap","text":"<p> </p>","location":"community/partnerships/facemap/#sustainability-roadmap-between-datajoint-elements-and-facemap"},{"title":"Aim","text":"<p>DataJoint Elements and Facemap are two neuroinformatics initiatives in active development. The projects develop independently yet they have complementary aims and overlapping user communities. This document establishes key processes for coordinating development and communications in order to promote integration and interoperability across the two ecosystems.</p>","location":"community/partnerships/facemap/#aim"},{"title":"Projects and Teams","text":"","location":"community/partnerships/facemap/#projects-and-teams"},{"title":"DataJoint","text":"<p>DataJoint Elements \u2014 https://datajoint.com/docs/elements/ \u2014 is a collection of   open-source reference database schemas and analysis workflows for neurophysiology   experiments, supported by DataJoint \u2014 https://datajoint.com/docs/core/ \u2014 an   open-source software framework. The project is funded by the NIH grant U24 NS116470   and led by Dr. Dimitri Yatsenko.</p> <p>The principal developer of DataJoint Elements and the DataJoint framework is the company DataJoint \u2014 https://datajoint.com.</p>","location":"community/partnerships/facemap/#datajoint"},{"title":"Facemap","text":"<p>Facemap - https://github.com/MouseLand/facemap \u2014 is a pipeline for processing imaging data. The project is funded by HHMI Janelia Research Campus and led by Dr. Carsen Stringer and Atika Syeda.</p> <p>The principal developers of Facemap are at the Janelia Research Campus.</p>","location":"community/partnerships/facemap/#facemap_1"},{"title":"General Principles","text":"","location":"community/partnerships/facemap/#general-principles"},{"title":"No obligation","text":"<p>The developers of the two ecosystems acknowledge that this roadmap document creates no contractual relationship between them but they agree to work together in the spirit of partnership to ensure that there is a united, visible, and responsive leadership and to demonstrate administrative and managerial commitment to coordinate development and communications.</p>","location":"community/partnerships/facemap/#no-obligation"},{"title":"Coordinated Development","text":"<p>The two projects will coordinate their development approaches to ensure maximum interoperability. This includes:</p> <ul> <li>coordinated use of terminology and nomenclatures</li> <li>support for testing infrastructure: unit testing and integration testing</li> <li>a coordinated software release process and versioning</li> <li>coordinated resolution of issues arising from joint use of the two tools</li> </ul>","location":"community/partnerships/facemap/#coordinated-development"},{"title":"Points of Contact","text":"<p>To achieve the aims of coordinated development, both projects appoint a primary point of contact (POC) to respond to questions relating to the integration and interoperability  of DataJoint Elements and Facemap.</p> <p>For 2022, the DataJoint Elements POC is Dr. Tolga Dincer (tolga@datajoint.com)</p> <p>For 2022, the Facemap POC is Dr. Carsen Stringer (stringerc@janelia.hhmi.org)</p>","location":"community/partnerships/facemap/#points-of-contact"},{"title":"Annual Review","text":"<p>To achieve the aims of coordinated development, the principal developers conduct a joint annual review of this roadmap document to ensure that the two programs are well integrated and not redundant. The contents and resolutions of the review will be made publicly available.</p>","location":"community/partnerships/facemap/#annual-review"},{"title":"Licensing","text":"<p>The two parties ensure that relevant software components are developed under licenses that avoid any hindrance to integration and interoperability between DataJoint Elements  and Facemap.</p>","location":"community/partnerships/facemap/#licensing"},{"title":"Development Roadmap","text":"<ul> <li>Mechanism to import Facemap results - Completed 2022 -  Element Facemap</li> </ul> <ul> <li>Mechanism to run Facemap within DataJoint Elements - Completed 2022 -  Element Facemap</li> </ul> <ul> <li>Tutorials on running DataJoint Element with Facemap - Under development </li> </ul> <ul> <li>Integration tests to verify loading Facemap data - Under development</li> </ul> <ul> <li>Integration tests to verify running Facemap - Under development</li> </ul>","location":"community/partnerships/facemap/#development-roadmap"},{"title":"Citation","text":"<p>If you use Facemap please cite  Stringer, Pachitariu, et al., Science 2019 in your publications.</p>","location":"community/partnerships/facemap/#citation"},{"title":"INCF","text":"<p>DataJoint is a company member of INCF.</p>","location":"community/partnerships/incf/"},{"title":"NWB","text":"","location":"community/partnerships/nwb/"},{"title":"Integrations between DataJoint Elements and Neurodata Without Borders","text":"<p> </p>","location":"community/partnerships/nwb/#integrations-between-datajoint-elements-and-neurodata-without-borders"},{"title":"Aim","text":"<p>DataJoint Elements and Neurodata Without Borders (NWB) are two neuroinformatics    initiatives in active development. The projects develop independently yet they have   complementary aims and overlapping user communities. This document establishes key   processes for coordinating development and communications in order to promote   integration and interoperability across the two ecosystems.</p>","location":"community/partnerships/nwb/#aim"},{"title":"Projects and Teams","text":"","location":"community/partnerships/nwb/#projects-and-teams"},{"title":"DataJoint","text":"<p>DataJoint Elements \u2014 https://datajoint.com/docs/elements/ \u2014 is a collection of   open-source reference database schemas and analysis workflows for neurophysiology   experiments, supported by DataJoint \u2014 https://datajoint.com/docs/core/ \u2014 an   open-source software framework. The project is funded by the NIH grant U24 NS116470   and led by Dr. Dimitri Yatsenko.</p> <p>The principal developer of DataJoint Elements and the DataJoint framework is the company DataJoint \u2014 https://datajoint.com.</p>","location":"community/partnerships/nwb/#datajoint"},{"title":"Neurodata without Borders (NWB)","text":"<p>NWB - https://www.nwb.org \u2014 is a data standard for neurophysiology, providing   neuroscientists with a common standard to share, archive, use, and build analysis   tools for neurophysiology data. The project is funded by the NIH grant U24 NS120057   and led by Dr. Oliver Rubel (Lawrence Berkeley National Laboratory) and Dr. Benjamin   Dichter (Catalyst Neuro).</p> <p>The principal developers of NWB are the Lawrence Berkeley National Laboratory and Catalyst Neuro.</p>","location":"community/partnerships/nwb/#neurodata-without-borders-nwb"},{"title":"General Principles","text":"","location":"community/partnerships/nwb/#general-principles"},{"title":"No obligation","text":"<p>The developers of the two ecosystems acknowledge that this roadmap document creates no contractual relationship between them but they agree to work together in the spirit of partnership to ensure that there is a united, visible, and responsive leadership and to demonstrate administrative and managerial commitment to coordinate development and communications.</p>","location":"community/partnerships/nwb/#no-obligation"},{"title":"Coordinated Development","text":"<p>The two projects will coordinate their development approaches to ensure maximum interoperability. This includes:</p> <ul> <li>coordinated use of terminology and nomenclatures</li> <li>support for testing infrastructure: unit testing and integration testing</li> <li>a coordinated software release process and versioning</li> <li>coordinated resolution of issues arising from joint use of the two tools</li> </ul>","location":"community/partnerships/nwb/#coordinated-development"},{"title":"Points of Contact","text":"<p>To achieve the aims of coordinated development, both projects appoint a primary point of contact (POC) to respond to questions relating to the integration and interoperability of DataJoint Elements and NWB.</p> <p>For 2022, the DataJoint Elements POC is Dr. Chris Brozdowski (cbroz@datajoint.com)</p> <p>For 2022, the NWB POC is Dr. Ryan Ly (Lawrence Berkeley National Laboratory)</p>","location":"community/partnerships/nwb/#points-of-contact"},{"title":"Annual Review","text":"<p>To achieve the aims of coordinated development, the principal developers conduct a joint annual review of this roadmap document to ensure that the two programs are well integrated and not redundant. The contents and resolutions of the review will be made publicly available.</p>","location":"community/partnerships/nwb/#annual-review"},{"title":"Licensing","text":"<p>The two parties ensure that relevant software components are developed under licenses that avoid any hindrance to integration and interoperability between DataJoint Elements workflows and NWB utilities.</p>","location":"community/partnerships/nwb/#licensing"},{"title":"Suite2p","text":"","location":"community/partnerships/suite2p/"},{"title":"Sustainability Roadmap between DataJoint Elements and Suite2p","text":"<p> </p>","location":"community/partnerships/suite2p/#sustainability-roadmap-between-datajoint-elements-and-suite2p"},{"title":"Aim","text":"<p>DataJoint Elements and Suite2p are two neuroinformatics initiatives in active   development. The projects develop independently yet they have complementary aims and   overlapping user communities. This document establishes key processes for   coordinating development and communications in order to promote integration and   interoperability across the two ecosystems.</p>","location":"community/partnerships/suite2p/#aim"},{"title":"Projects and Teams","text":"","location":"community/partnerships/suite2p/#projects-and-teams"},{"title":"DataJoint","text":"<p>DataJoint Elements \u2014 https://datajoint.com/docs/elements/ \u2014 is a collection of   open-source reference database schemas and analysis workflows for neurophysiology   experiments, supported by DataJoint \u2014 https://datajoint.com/docs/core/ \u2014 an   open-source software framework. The project is funded by the NIH grant U24 NS116470   and led by Dr. Dimitri Yatsenko.</p> <p>The principal developer of DataJoint Elements and the DataJoint framework is the company DataJoint \u2014 https://datajoint.com.</p>","location":"community/partnerships/suite2p/#datajoint"},{"title":"Suite2p","text":"<p>Suite2p \u2014 https://www.suite2p.org \u2014 is a pipeline for processing calcium imaging   data. The project is funded by HHMI Janelia Research Campus and led by Dr. Carsen   Stringer and Dr. Marius Pachitariu.</p> <p>The principal developers of Suite2p are at the Janelia Research Campus.</p>","location":"community/partnerships/suite2p/#suite2p_1"},{"title":"General Principles","text":"","location":"community/partnerships/suite2p/#general-principles"},{"title":"No obligation","text":"<p>The developers of the two ecosystems acknowledge that this roadmap document creates no contractual relationship between them but they agree to work together in the spirit of partnership to ensure that there is a united, visible, and responsive leadership and to demonstrate administrative and managerial commitment to coordinate development and communications.</p>","location":"community/partnerships/suite2p/#no-obligation"},{"title":"Coordinated Development","text":"<p>The two projects will coordinate their development approaches to ensure maximum interoperability. This includes:</p> <ul> <li>coordinated use of terminology and nomenclatures</li> <li>support for testing infrastructure: unit testing and integration testing</li> <li>a coordinated software release process and versioning</li> <li>coordinated resolution of issues arising from joint use of the two tools</li> </ul>","location":"community/partnerships/suite2p/#coordinated-development"},{"title":"Points of Contact","text":"<p>To achieve the aims of coordinated development, both projects appoint a primary point of contact (POC) to respond to questions relating to the integration and interoperability  of DataJoint Elements and Suite2p.</p> <p>For 2022, the DataJoint Elements POC is Dr. Tolga Dincer (tolga@datajoint.com)</p> <p>For 2022, the Suite2p POC is Dr. Carsen Stringer (stringerc@janelia.hhmi.org)</p>","location":"community/partnerships/suite2p/#points-of-contact"},{"title":"Annual Review","text":"<p>To achieve the aims of coordinated development, the principal developers conduct a joint annual review of this roadmap document to ensure that the two programs are well integrated and not redundant. The contents and resolutions of the review will be made publicly available.</p>","location":"community/partnerships/suite2p/#annual-review"},{"title":"Licensing","text":"<p>The two parties ensure that relevant software components are developed under licenses that avoid any hindrance to integration and interoperability between DataJoint Elements and Suite2p.</p>","location":"community/partnerships/suite2p/#licensing"},{"title":"Development Roadmap","text":"<ul> <li>Mechanism to import Suite2p results - Completed 2021 -  Element Interface Suite2p module</li> </ul> <ul> <li>Mechanism to run Suite2p within DataJoint Element - Completed 2022 -  Element Calcium Imaging</li> </ul> <ul> <li>Tutorials on running DataJoint Element with Suite2p - Completed 2021 - Workflow Calcium Imaging Jupyter notebooks</li> </ul> <ul> <li>Integration tests to verify loading Suite2p data - Completed 2021 -  Pytests</li> </ul> <ul> <li>Integration tests to verify running Suite2p - Completed 2022 -  Pytests</li> </ul>","location":"community/partnerships/suite2p/#development-roadmap"},{"title":"Citation","text":"<p>If you use Suite2p please cite  Pachitariu et al., bioRxiv 2017 in your publications.</p>","location":"community/partnerships/suite2p/#citation"},{"title":"Core","text":"<p>DataJoint Core projects are fully open-source and are built to develop, define, manage, and visualize data pipelines. Below are the projects that make up the family of core open-source projects.</p>","location":"core/"},{"title":"APIs","text":"<ul> <li> DataJoint Python <p>A low-level client for managing data pipelines.</p> <p> New docs coming soon!  In the meantime, refer to our legacy docs</p> </li> </ul> <ul> <li> DataJoint MATLAB <p>A low-level client for managing data pipelines.</p> <p> New docs coming soon!  In the meantime, refer to our legacy docs.</p> </li> </ul> <ul> <li> Pharus <p>Expose data pipelines via a REST interface.</p> <p> New docs coming soon!  In the meantime, refer to our legacy docs.</p> </li> </ul>","location":"core/#apis"},{"title":"Web GUIs","text":"<ul> <li> LabBook <p>Data entry and data model browsing for data pipelines.</p> <p> New docs coming soon!  In the meantime, refer to our legacy docs.</p> </li> </ul> <ul> <li> SciViz <p>A visualization framework for making low-code web apps for    data pipelines.</p> <p> New docs coming soon!</p> </li> </ul>","location":"core/#web-guis"},{"title":"Container Images","text":"<pre><code>graph\n  %% Give short names\n  dj[\"datajoint/datajoint\"]\n  base[\"datajoint/djbase\"]\n  lab[\"datajoint/djlab\"]\n  hub[\"datajoint/djlabhub\"]\n  test[\"datajoint/djtest\"]\n  conda3[\"datajoint/miniconda3\"]\n  mysql[\"datajoint/mysql\"]\n  %% Define connections\n  conda3 --&gt; base --&gt; test;\n  base --&gt; dj;\n  base --&gt; lab --&gt; hub;\n  %% Add all to class\n  class dj,base,lab,hub,test,conda3,mysql boxes;\n  classDef boxes stroke:#333; %% Grey stroke for class</code></pre>  <ul> <li> datajoint/mysql <p>An optimized, MySQL backend for data pipelines.</p> <p> New docs coming soon!</p> </li> </ul> <ul> <li> datajoint/miniconda3 <p>A minimal Python image with conda.</p> <p> New docs coming soon!  In the meantime, refer to our legacy docs.</p> </li> </ul> <ul> <li> datajoint/djbase <p>Adds only dependencies for managing data pipelines.</p> <p> New docs coming soon!  In the meantime, refer to our legacy docs.</p> </li> </ul> <ul> <li> datajoint/djtest <p>Adds testing tools like pytest.</p> <p> New docs coming soon!  In the meantime, refer to our legacy docs.</p> </li> </ul> <ul> <li> datajoint/datajoint <p>Official image for managing data pipelines.</p> <p> New docs coming soon!</p> </li> </ul> <ul> <li> datajoint/djlab <p>Adds a local Jupyter Lab environment.</p> <p> New docs coming soon!  In the meantime, refer to our legacy docs.</p> </li> </ul> <ul> <li> datajoint/djlabhub <p>Adds a client to allow hosting with Jupyter Hub.</p> <p> New docs coming soon!  In the meantime, refer to our legacy docs.</p> </li> </ul>","location":"core/#container-images"},{"title":"Frequently Asked Questions","text":"","location":"core/faq/"},{"title":"How do I use DataJoint with a GUI?","text":"<ol> <li> <p>The DataJoint Works platform is set up as a fully managed service, that will host and process data for you.</p> </li> <li> <p>LabBook is an open source project for data entry.</p> </li> </ol>","location":"core/faq/#how-do-i-use-datajoint-with-a-gui"},{"title":"Does DataJoint support other programming languages?","text":"<p>DataJoint Python and [Matlab] (https://datajoint.com/docs/core/datajoint-matlab/) APIs are both actively supported. Previous projects implemented some DataJoint features in Julia and Rust. DataJoint's data model and data representation are largely language independent, which means that any language with a DataJoint client can work with a data pipeline defined in any other language. DataJoint clients for other programming languages will be implemented based on demand. All languages must comply to the same data model and computation approach as defined in DataJoint: a simpler relational data model.</p>","location":"core/faq/#does-datajoint-support-other-programming-languages"},{"title":"Can I use DataJoint with my current database?","text":"<p>Researchers use many different tools to keep records, from simple formalized file heirarchies to complete software packages for colony management and standard file types like NWB. Existing projects have built interfaces with many such tools, such as PyRAT. The only requirement for interface is that tool has an open API. Contact Support@DataJoint.com with inquiries. The DataJoint team will consider development requests based on community demand.</p>","location":"core/faq/#can-i-use-datajoint-with-my-current-database"},{"title":"Is DataJoint an ORM?","text":"<p>Programmers are familiar with object-relational mappings (ORM) in various programming languages. Python in particular has several popular ORMs such as SQLAlchemy and Django ORM. The purpose of ORMs is to allow representations and manipulations of objects from the host programming language as data in a relational database. ORMs allow making objects persistent between program executions by creating a bridge (i.e., mapping) between the object model used by the host language and the relational model allowed by the database. The result is always a compromise, usually toward the object model. ORMs usually forgo key concepts, features, and capabilities of the relational model for the sake of convenient programming constructs in the language.</p> <p>In contrast, DataJoint implements a data model that is a refinement of the relational data model without compromising its core principles of data representation and queries. DataJoint supports data integrity (entity integrity, referential integrity, and group integrity) and provides a fully capable relational query language. DataJoint remains absolutely data-centric, with the primary focus on the structure and integrity of the data pipeline. Other ORMs are more application-centric, primarily focusing on the application design while the database plays a secondary role supporting the application with object persistence and sharing.</p>","location":"core/faq/#is-datajoint-an-orm"},{"title":"What is the difference between DataJoint and Alyx?","text":"<p>Alyx is an experiment management database application developed in Kenneth Harris' lab at UCL.</p> <p>Alyx is an application with a fixed pipeline design with a nice graphical user interface. In contrast, DataJoint is a general-purpose library for designing and building data processing pipelines.</p> <p>Alyx is geared towards ease of data entry and tracking for a specific workflow (e.g. mouse colony information and some pre-specified experiments) and data types. DataJoint could be used as a more general purposes tool to design, implement, and execute processing on such workflows/pipelines from scratch, and DataJoint focuses on flexibility, data integrity, and ease of data analysis. The purposes are partly overlapping and complementary. The International Brain Lab project is developing a bridge from Alyx to DataJoint, hosted as an open-source project. It implements a DataJoint schema that replicates the major features of the Alyx application and a synchronization script from an existing Alyx database to its DataJoint counterpart.</p>","location":"core/faq/#what-is-the-difference-between-datajoint-and-alyx"},{"title":"Glossary","text":"","location":"core/glossary/"},{"title":"Glossary","text":"<p>There are many terms that are reused throughout the documentation that we feel important to define together. We've taken careful consideration to be consistent. Below you will find how we've understood and use these terms.</p>     Term Definition     DAG directed acyclic graph (DAG) is a set of nodes and unidirectional edges with no cyclic dependencies. No child node points to a parent node.   data pipeline formal definition of a directed acyclic graph (DAG) of processes that achieves the DataJoint Mantra   DataJoint a software framework for database programming directly from matlab and python. Thanks to its support of automated computational dependencies, DataJoint serves as a workflow management system.   DataJoint Elements software modules implementing portions of experiment workflows designed for ease of integration into diverse custom workflows.   DataJoint pipeline the data schemas and transformations underlying a DataJoint workflow. DataJoint allows defining code that specifies both the workflow and the data pipeline, and we have used the words \"pipeline\" and \"workflow\" almost interchangeably.   DataJoint schema a software module implementing a portion of an experiment workflow. Includes database table definitions, dependencies, and associated computations.   djHub our team's internal platform for delivering cloud-based infrastructure to support online training resources, validation studies, and collaborative projects.   foreign key a field that is linked to another table's primary key.   primary key the subset of table attributes that uniquely identify each entity in the table.   secondray attribute any field in a table not in the primary key.   workflow a formal representation of the steps for executing an experiment from data collection to analysis. Also the software configured for performing these steps. A typical workflow is composed of tables with inter-dependencies and processes to compute and insert data into the tables.","location":"core/glossary/#glossary"},{"title":"Mantra","text":"<p>The DataJoint Mantra consists of three main objectives:</p> <ul> <li>Simplify your data queries through an intuitive query language.</li> <li>Make automated, reproducible computation by integrating   computation with the data model.</li> <li>Ensure validity of your data through referential integrity.</li> </ul>","location":"core/concepts/mantra/"},{"title":"Query Language","text":"<p>Writing good, optimized SQL queries can be difficult and often becomes a barrier for individuals lacking experience in computer science and programming. That said, we don't feel this should discourage the use of databases. Databases help to structure our daily lives which streamlines the time required to glean insights and build robust applications from truth. SQL is powerful but requires practice which we feel is the real fault in the language.</p> <p>To address this, the DataJoint query language serves as a query builder and optimizer for SQL. It leverages the stack's own operator precedence and combines it with both operator overloading and  SQL algebra to achieve a more intuitive experience. Additionally, interoperability between Python and MATLAB is crucial due to the diversity of tools available to scientists. So much so that this is a guiding principle in FAIR.</p> <p>Case in point, here is a comparison of equivalent queries:</p> <p>SQL:</p> <pre><code>SELECT *\nFROM `shapes`.`rectangle`\nNATURAL JOIN `shapes`.`area`\nWHERE (\n    (`shape_area`=8) AND (`shape_height`=2)\n);\n</code></pre> DataJoint (Python)DataJoint (MATLAB)   <pre><code>Rectangle * Area &amp; dict(shape_height=2, shape_area=8)\n</code></pre>   <pre><code>shapes.Rectangle * shapes.Area &amp; struct('shape_height', 2, 'shape_area', 8)\n</code></pre>","location":"core/concepts/mantra/#query-language"},{"title":"Reproducible Computation","text":"<p>Reproducibility is a key concept within the scientific community since research is largely conducted, shared, and reviewed in the public domain. This is necessary to independently validate discoveries and have others support new findings. Such a practice is well advocated in the scientific community as open science.</p> <p>Yet, reliably reproducing computed results of others has proven difficult since there are many factors that affect the determinism of a process e.g. hardware, software environment, scripts, input data, seeding, etc.</p> <p>DataJoint pipelines address these challenges by allowing computation to be defined such that they are associated with an entity. Drawing relationships between many entities we can create a DAG that describes a compute workflow as an  entity-relationship model.</p> <p>For instance, an entity such as <code>Area</code> could represent the computed value of a parent entity, <code>Rectangle</code>. Therefore, we feel it should be reasonable when defining <code>Area</code> to include the specification of a computation that automates how <code>Area</code> is generated based on relation to <code>Rectangle</code>.</p>","location":"core/concepts/mantra/#reproducible-computation"},{"title":"Referential Integrity","text":"<p>Referential integrity is the concept of keeping all your data consistent and up-to-date. The goal is to ensure data pipelines always reflect the truth of how data was created.</p> <p>In the realm of databases, entities can be related to one another through  foreign keys. However, our opinionated view is that foreign keys on primary keys should enforce the contraint.</p> <p>What this means is that our data model always reflects the truth. When a parent entity is removed, all child computed values will also be removed since they no longer have meaning without the subject. There is not a clear way to reproduce the results otherwise.</p> <p>An important consequence to note is that deletes take longer as a result since they must be cascaded down to all the descendants. We believe this to be a feature as it is the behavior most inline with typical expectations. Deletes should be done cautiously. </p>","location":"core/concepts/mantra/#referential-integrity"},{"title":"DataJoint 101","text":"<p>Migrating from a unique system to a shared relational database can be challenging, but  there are a lot of materials to help you get started.</p>","location":"core/concepts/getting-started/"},{"title":"Hands-on learning","text":"<p>First, it helps to see a data pipeline in action. DataJoint's Works platform offers graphical user interfaces for our most common pipelines via  Elements. Many of the Elements also have Jupyter notebooks you can run on your own machine, to start using Python or Matlab interfaces. Follow along with the general user instructions to get started.</p>","location":"core/concepts/getting-started/#hands-on-learning"},{"title":"Why data pipelines?","text":"<p>DataJoint is one way to define a comprehensive data pipelines. This is the full set of instructions that takes raw experimental data and (a) formats it in an accessible, concepts-first manner, and (b) processes it into publication-ready analyses. This will involve making some decisions about how you want to organize your data before importing it into a pipeline, but a well-designed pipeline can support and automate many projects and expedite the research process.</p> <p>For more about the why, see the DataJoint Mantra.</p>","location":"core/concepts/getting-started/#why-data-pipelines"},{"title":"Data Pipelines","text":"<p>DataJoint is a free open-source framework for creating scientific data pipelines directly from Python or MATLAB (or both). The data are stored in a language-independent way, accessible via both Python and MATLAB. In DataJoint, a data pipeline is a sequence of steps (see also DAGs) with integrated data storage at each step, as defined by the tables. </p> <p>Formally, a scientific data pipeline is a collection of processes and systems for organizing the data, computations, and workflows used by a research group as they jointly perform complex sequences of data acquisition, processing, and analysis. A full-featured data pipeline framework may also be described as a  scientific workflow system.</p> <p>In practice, a data pipeline is some definition of how should flow from its origin  through publication-ready analysis. In its most basic form, this could be a set of  spreadsheets and a checklist of each processing step that should be applied to each.</p> <p>DataJoint APIs give researches a set of tools for defining and automating all the steps of data management and processing for an experiment across the whole team, and play a  key role in a broader pipeline.</p> <pre><code>flowchart LR\n  subgraph repo[\"&lt;b&gt;Data Repository&lt;/b&gt;\n                #8226;Deposit &amp; Retrieve\n                #8226;Access Control\"]\n    direction TB\n  end\n  subgraph empty1[\" \"] %% Empty subgraphs prevent overlapping titles\n    direction LR\n    style empty1 fill:none, stroke-dasharray: 0 1\n    emptyone[\" \"]\n    repo\n  end\n  subgraph db[\"&lt;center&gt;&lt;b&gt;Database&lt;/b&gt;&lt;/center&gt;#8226;Structure &lt;i&gt;(schema)\n              #8226;Data Integrity \n              &lt;i&gt;(identity, references, groups)&lt;/i&gt;\n              #8226;Queries\"]\n    direction LR\n    empty1\n  end\n  subgraph pipe[\"&lt;center&gt;&lt;b&gt;Data Pipeline&lt;/b&gt;&lt;/center&gt;#8226;Workflow\n                #8226;Computation\"]\n    class pipe vertical-align:bottom;\n    direction LR\n    emptytwo[\" \"]\n    db\n  end\n  class emptyone,emptytwo void;\n  classDef void fill:none, stroke-dasharray: 0 1\n  class repo,db,pipe boxes;\n  classDef boxes fill:#ddd, stroke:#333;</code></pre>","location":"core/concepts/getting-started/data-pipelines/"},{"title":"Data repositories","text":"<p>A shared data repository gives all team members access to the data. This might include a collection of files with standard naming conventions, organized into folders and sub-folders. Or, a data repository might be a collection of  S3 buckets hosted on a cloud server.</p>","location":"core/concepts/getting-started/data-pipelines/#data-repositories"},{"title":"Database systems","text":"<p>Databases are a form of data repository, with additional capabilities:</p> <ol> <li> <p>Define, communicate, and enforce structure in the stored data. </p> </li> <li> <p>Maintain data integrity: correct identification of data and consistent cross-references, dependencies, and groupings among the data. </p> </li> <li> <p>Support queries that retrieve various cross-sections and transformation of the  deposited data.</p> </li> </ol> <p>Most scientists have some familiarity with these concepts, for example the notion of maintaining consistency between data and the metadata that describes it, or applying a filter to an Excel spreadsheet to retrieve specific subsets of information. However, usually the more advanced concepts involved in building and using relational databases fall under the specific expertise of data scientists.</p>","location":"core/concepts/getting-started/data-pipelines/#database-systems"},{"title":"Data pipelines","text":"<p>Data pipeline frameworks may include all the features of a database system along with additional functionality:</p> <ol> <li> <p>Integrating computations to perform analyses and manage intermediate results in a   principled way.</p> </li> <li> <p>Supporting distributed computations without conflict.</p> </li> <li> <p>Defining, communicating, and enforcing workflow, making clear the sequence of  steps that must be performed for data entry, acquisition, and processing.</p> </li> </ol> <p>The informal notion of an analysis \"workflow\" will be familiar to most scientists. This could include the the logistical difficulties associated making sure each file is  preprocessed, while distributing the load across multiple team members or compute  resources.</p>","location":"core/concepts/getting-started/data-pipelines/#data-pipelines_1"},{"title":"Dependencies","text":"<p>Dependencies, in short, are the links between data tables.</p> <p>In the context of a broader pipeline, individual tables will be related. Individual fields in one table will derive some of their meaning from entities in other tables. For example, a subject identifier in a Session table will be associated with more detailed subject-level information in a Subject table.</p> <p>A foreign key defines a dependency of entities in one table on entities in another within a schema. Dependencies provide entities in one table with access to data in another table and establish certain constraints on entities containing a foreign key. A DataJoint pipeline, including the dependency relationships established by foreign keys, can be visualized as a graph with nodes and edges, in a diagram. </p>","location":"core/concepts/getting-started/dependencies/"},{"title":"Defining a dependency","text":"<p>Foreign keys are defined with arrows <code>-&gt;</code> in the table definition, pointing to another table.</p> <p>In the example below, there are three foreign keys, including one within the primary key above the <code>---</code>.</p> <pre><code>## brain slice\n-&gt; mp.Subject\nslice_id        : smallint       ## slice number within subject\n---\n-&gt; mp.BrainRegion\n-&gt; mp.Plane\nslice_date        : date                 ## date of the slicing (not patching)\nthickness         : smallint unsigned    ## slice thickness in microns\nexperimenter      : varchar(20)          ## person who performed this experiment\n</code></pre> <p>You can examine the resulting table heading with the the following command</p> PythonMatlab   <pre><code>mp.BrainSlice.heading\n</code></pre>   <pre><code>show(mp.BrainSlice)\n</code></pre>    <p>The heading of <code>Slice</code> may look something like</p> <pre><code>subject_id          : char(8)            ## experiment subject id\nslice_id            : smallint           ## slice number within subject\n---\nbrain_region        : varchar(12)        ## abbreviated name for brain region\nplane               : varchar(12)        ## plane of section\nslice_date          : date               ## date of the slicing (not patching)\nthickness           : smallint unsigned  ## slice thickness in microns\nexperimenter        : varchar(20)        ## person who performed this experiment\n</code></pre> <p>This displayed heading reflects the actual attributes in the table. The foreign keys have been replaced by the primary key attributes of the referenced tables, including their data types and comments.</p>  How it works under the hood <p>The foreign key <code>-&gt; A</code> in the definition of table <code>B</code> has the following effects:</p> <ol> <li>The primary key attributes of <code>A</code> are made part of <code>B</code>'s definition.</li> <li>A referential constraint is created in <code>B</code> with reference to <code>A</code>.</li> <li>If one does not already exist, an index is created to speed up     searches in <code>B</code> for matches to <code>A</code>. (The reverse search is already     fast because it uses the primary key of <code>A</code>.)</li> </ol> <p>A referential constraint means that an entity in <code>B</code> cannot exist without a matching entity in <code>A</code>. Matching means attributes in <code>B</code> that correspond to the primary key of <code>A</code> must have the same values. An attempt to insert an entity into <code>B</code> that does not have a matching counterpart in <code>A</code> will fail. Conversely, deleting an entity from <code>A</code> that has matching entities in <code>B</code> will result in the deletion of those matching entities and so forth, recursively, downstream in the pipeline.</p>  <p>When <code>B</code> references <code>A</code> with a foreign key, one can say that <code>B</code> depends on <code>A</code>. In DataJoint terms, <code>B</code> is the dependent table and <code>A</code> is the referenced table with respect to the foreign key from <code>B</code> to <code>A</code>.</p>  Relational database theory <p>The usage of the words \"depends\" and \"dependency\" here should not be confused with the unrelated concept of functional dependencies that is used to define normal forms.</p>","location":"core/concepts/getting-started/dependencies/#defining-a-dependency"},{"title":"Diagrams","text":"<p>Diagrams are a great way to visualize all or part of a pipeline and understand the flow of data. DataJoint diagrams are based on entity relationship diagram (ERD), with some minor departures fom this standard. </p> <p>Here, tables are depicted as nodes and dependencies as directed edges between them. The <code>draw</code> method plots the graph, with many other methods ( Python, Matlab) to save or adjust the output.</p> <p>Because DataJoint pipelines are directional (see DAG), the tables at the top will need to be populated first, followed by those tables one step below and so forth until the last table is populated at the bottom of the pipeline. The top of the pipeline tends to be dominated by Lookup and manual tables. The middle has many imported tables, and the bottom has computed tables.</p>","location":"core/concepts/getting-started/diagrams/"},{"title":"Notation","text":"<p>DataJoint uses the following conventions:</p> <ul> <li>Tables are indicated as nodes in the graph. The     corresponding class name is indicated by each node.</li> </ul> <ul> <li>Table type is indicated by colors and symbols, with some     differences across Python and Matlab: <p>- Lookup: gray, rectangle or asterisk</p> <p>- Manual: green, rectangle or square</p> <p>- Imported: blue, circle or oval</p> <p>- Computed: red, rectangle or star</p> <p>- Part: black dot with smaller font or black text</p> </li> </ul> <ul> <li>Dependencies indicated as edges in the graph and always     directed downward (see DAG)</li> </ul> <ul> <li>Dependency type is indicated by the line.<p>- Solid lines: The foreign key in the     primary key.</p> <p>- Dashed lines: The foreign key outside the     primary key. </p> <p>- Thick line: The foreign key the only item in     the primary key. This is a 1-to-1 relationship.</p> <p>- Dot on the line: The foreign key was renamed     via the projection</p> </li> </ul>","location":"core/concepts/getting-started/diagrams/#notation"},{"title":"Example","text":"<p>The following diagram example is an approximation of a DataJoint diagram using Mermaid.</p>  <pre><code>  flowchart TB\n    Mouse --&gt; \n    Session ==&gt; Scan %% Thick line\n    Scan --&gt; Alignment((Alignment)) --&gt; Segmentation --&gt; Trace((Trace)) --&gt; RF((RF))\n    Scan --&gt; Stimulus --&gt; RF --&gt; Field\n    SegmentationMethod -.-&gt; Segmentation((Segmentation)) %% Circle shape\n    %% class list,of,nodes class-name\n    class Alignment,Mouse,RF,Scan,Segmentation,SegmentationMethod,Session,Stimulus,Trace,Field all;\n    class Mouse,Scan,Session,Stimulus manual;\n    class Segmentation,Trace,RF compute;\n    class SegmentationMethod lookup;\n    class Alignment import;\n    class Field part;\n    %% classDef class-name option1:var1,option2:var2;\n    classDef all stroke:#333;   %% Grey stroke around all tables\n    classDef manual fill:#060;  %% Green manual tables\n    classDef compute fill:#600; %% Red compute tables\n    classDef import fill:#006;  %% Blue import tables\n    classDef lookup fill:#ddd;  %% Grey lookup tables\n    classDef part fill:#FFF;    %% White part tables\n    %% Above colors were chosen to be compatible with #00a0df as text</code></pre> <p>Here, we see ...</p> <ol> <li> <p>A 1-to-1 relationship between Session and Scan, as designated by the thick edge.</p> </li> <li> <p>A non-primary foreign key linking SegmentationMethod and Segmentation</p> </li> <li> <p>Manual tables for Mouse, Session, Scan, and Stimulus.</p> </li> <li> <p>A Lookup table: SegmentationMethod</p> </li> <li> <p>An Imported table: Alignment</p> </li> <li> <p>Several Computed tables: Segmentation, Trace, and RF</p> </li> <li> <p>A part table: Field</p> </li> </ol> <p>For examples calling <code>Diagram</code> in Python and Matlab, please visit the documentation for the respective API.</p>","location":"core/concepts/getting-started/diagrams/#example"},{"title":"Table Definitions","text":"<p>DataJoint models data as sets of entities (rows) with shared attributes (columns or fields). These are visualized as tables with rows and columns. Each row represents a single entity and the values of all of its attributes. Each column represents a single attribute with a name and a datatype. Unlike rows in a spreadsheet, entities in DataJoint don't have names or numbers: they can only be identified by the values of their attributes. Defining a table means defining the names and datatypes of the attributes as well as the constraints to be applied to those attributes.</p> <p>To make it easy to work with tables in Python and Matlab, DataJoint APIs create a separate class for each table1. For example, the class <code>experiment.Subject</code> in the DataJoint client language may correspond to the table called <code>subject</code> on the database server. Each table class must inherit from one of the table tier classes.</p>  Valid table names <p>Note that in both MATLAB and Python, the class names must follow the CamelCase compound word notation:</p> <ul> <li>start with a capital letter and</li> <li>contain only alphanumerical characters (no underscores).</li> </ul> <p>Examples of valid class names:</p> <p><code>TwoPhotonScan</code>, <code>Scan2P</code>, <code>Ephys</code>, <code>MembraneVoltage</code></p> <p>Invalid class names:</p> <p><code>Two_photon_Scan</code>, <code>twoPhotonScan</code>, <code>2PhotonScan</code>, <code>membranePotential</code>, <code>membrane_potential</code></p>","location":"core/concepts/getting-started/table-definitions/"},{"title":"Table Definition Syntax","text":"<p>Both MATLAB and Python use the same syntax define tables. A table definition consists of one or more lines. Each line can be one of the following:</p> <ul> <li><code>#</code>: The optional first line may provide a description     of the table's purpose. </li> </ul> <ul> <li>A field (i.e., attribute) definition can take any of the following forms (see     also valid datatypes):<p>- <code>name : datatype</code>    - <code>name : datatype # comment</code>   - <code>name = default : datatype</code>    - <code>name = default : datatype  # comment</code></p> </li> </ul> <ul> <li>The divider <code>---</code> (at least three hyphens) separates      primary key attributes above from      secondary attributes below.</li> </ul> <ul> <li>A foreign key in the format <code>-&gt; ReferencedTable</code>.</li> </ul> <p>For example, the table for Persons may have the following definition:</p> <p><pre><code># Persons in the lab\nusername :  varchar(16)   #  username in the database\n---\nfull_name  : varchar(255)\nstart_date :  date   # date when joined the lab\n</code></pre> <pre><code>    # Mice\n    mouse_id: int            # unique mouse id\n    ---\n    dob: date                # mouse date of birth\n    sex: enum('M', 'F', 'U') # sex of mouse - Male, Female, or Unknown\n</code></pre></p> <p>This will define the table with attributes <code>mouse_id</code>, <code>dob</code>, and <code>sex</code>, in which <code>mouse_id</code> is the primary key.</p>","location":"core/concepts/getting-started/table-definitions/#table-definition-syntax"},{"title":"Attribute names","text":"<p>Attribute names must be in lowercase and must start with a letter. They can only contain alphanumerical characters and underscores. The attribute name cannot exceed 64 characters.</p>  Valid attribute names <p>Attribute names should appear in snake case, with underscores and not  spaces. </p> <ul> <li>Valid attribute names: <code>first_name</code>, <code>two_photon_scan</code>, <code>scan_2p</code>,      <code>two_photon_scan_</code></li> <li>Invalid attribute names: <code>firstName</code>, <code>first name</code>, <code>2photon_scan</code>,      <code>two-photon_scan</code>, <code>TwoPhotonScan</code></li> </ul>  <p>Ideally, attribute names should be unique across all tables that are likely to be used in queries together. For example, tables often have attributes representing the start times of sessions, recordings, etc. Such attributes must be uniquely named in each table, such as <code>session_start_time</code> or <code>recording_start_time</code>.</p>","location":"core/concepts/getting-started/table-definitions/#attribute-names"},{"title":"Default values","text":"<p>Secondary attributes can be given default values. A default value will be used for an attribute if no other value is given at the time the entry is  inserted into the table. Generally, default  values are numerical values or character strings. Default values for dates must be given as strings as well, contained within quotes (with the exception of <code>CURRENT_TIMESTAMP</code>).  Primary key attributes cannot have default values (with the exceptions of <code>auto_increment</code> and <code>CURRENT_TIMESTAMP</code> attributes; see  primary keys.</p> <p>An attribute with a default value of <code>NULL</code> is called a nullable attribute, which may be absent in some entities. Nullable attributes should not be used to indicate that an attribute is inapplicable to some entities in a table (see  normalization). Nullable attributes should be used sparingly to indicate optional rather than inapplicable attributes that still apply to all entities in the table. <code>NULL</code> is a special literal value and does not need to be enclosed in quotes.</p> <p>Here are some examples of attributes with default values:</p> <pre><code>    failures = 0 : int\n    due_date = \"2020-05-31\" : date\n    additional_comments = NULL : varchar(256)\n</code></pre>","location":"core/concepts/getting-started/table-definitions/#default-values"},{"title":"Changing the Definition","text":"<p>Once the table is created in the database, the definition string has no effect. Just changing the definition string in the class of an existing table will make any corresponding changes on the database definition. </p> <p>To change the table in the database, one can either...</p> <ol> <li> <p>Drop the existing table, deleting the entire contents, and then declare a new adjusted table.</p> </li> <li> <p>Or alter the table definition. Altering is limited to  secondary attributes and should be done with  caution, as it may impact existing data.</p> </li> </ol> <p>In the initial phases of designing a pipeline, it's best to experiment with variations of the design before populating it with substantial amounts of data.</p>","location":"core/concepts/getting-started/table-definitions/#changing-the-definition"},{"title":"Reverse-engineering the Definition","text":"<p>DataJoint objects provide the <code>describe</code> method, which displays the table definition used to define the table when it was created in the database. This definition may differ from the definition string of the class if the definition string has been edited after creation of the table (see above).</p> PythonMatlab   <pre><code>s = lab.User.describe()\n</code></pre>   <pre><code>s = describe(lab.User)\n</code></pre>      <ol> <li> <p>Computer programmers refer to this concept as  object-relational mapping.\u00a0\u21a9</p> </li> </ol>","location":"core/concepts/getting-started/table-definitions/#reverse-engineering-the-definition"},{"title":"Common Commands","text":"","location":"core/concepts/query-lang/common-commands/"},{"title":"Insert","text":"<p>Data entry is as easy as providing the appropriate data structure to a permitted table.</p> <p>Given the following table definition, we can insert data as follows.</p> <pre><code>    mouse_id: int            # unique mouse id\n    ---\n    dob: date                # mouse date of birth\n    sex: enum('M', 'F', 'U') # sex of mouse - Male, Female, or Unknown\n</code></pre> PythonMatlab   <pre><code>mouse.insert1( (0, '2017-03-01', 'M') ) # Single entry\ndata = [\n  (1, '2016-11-19', 'M'),\n  (2, '2016-11-20', 'U'),\n  (5, '2016-12-25', 'F')\n]\nmouse.insert(data) # Multi-entry\n</code></pre>   <pre><code>insert(tutorial.Mouse, {0, '2017-03-01', 'M'} ) # Single entry\ndata = [\n  {1, '2016-11-19', 'M'},\n  {2, '2016-11-20', 'U'},\n  {5, '2016-12-25', 'F'}\n]\n\n% now insert all at once\ninsert(tutorial.Mouse, data)\n</code></pre>","location":"core/concepts/query-lang/common-commands/#insert"},{"title":"Make","text":"<p>The <code>make</code> method populates automated tables from inserted data. Read more in the full article here</p>","location":"core/concepts/query-lang/common-commands/#make"},{"title":"Fetch","text":"<p>Data queries in DataJoint comprise two distinct steps:</p> <ol> <li>Construct the <code>query</code> object to represent the required data using     tables and operators.</li> <li>Fetch the data from <code>query</code> into the workspace of the host language.</li> </ol> <p>Note that entities returned by <code>fetch</code> methods are not guaranteed to be sorted in any particular order unless specifically requested. Furthermore, the order is not guaranteed to be the same in any two queries, and the contents of two identical queries may change between two sequential invocations unless they are wrapped in a transaction. Therefore, if you wish to fetch matching pairs of attributes, do so in one <code>fetch</code> call.</p> PythonMatlab   <pre><code>data = query.fetch()\n</code></pre>   <pre><code>result = query.fetch(query, 'attr1')\n</code></pre>    <p>For more examples in Python or Matlab, please visit the respective API documentation. </p>","location":"core/concepts/query-lang/common-commands/#fetch"},{"title":"Drop","text":"<p>The <code>drop</code> method completely removes a table from the database, including its definition. It also removes all dependent tables, recursively. DataJoint will first display the tables being dropped and the number of entities in each before prompting the user for confirmation to proceed.</p> <p>The <code>drop</code> method is often used during initial design to allow altered table definitions to take effect.</p> PythonMatlab   <pre><code># drop the Person table from its schema\nPerson.drop()\n</code></pre>   <pre><code>% drop the Person table from the lab schema\ndrop(lab.Person)\n</code></pre>","location":"core/concepts/query-lang/common-commands/#drop"},{"title":"Diagrams","text":"<p>The <code>Diagram</code> or <code>ERD</code> command can help you visualize your pipeline, or understand an existing pipeline. </p> PythonMatlab   <pre><code>import datajoint as dj\nschema = dj.Schema('my_database')\ndj.Diagram(schema).draw()\n</code></pre>   <p>The schema object for a package can be obtained using its <code>getSchema</code> function. (See <code>schema</code>.)</p> <pre><code>draw(dj.ERD(seq.getSchema))   % draw the diagram\n</code></pre>    <p>For more information about diagrams, see this article. For more examples in Python or Matlab, please visit the respective API documentation. </p>","location":"core/concepts/query-lang/common-commands/#diagrams"},{"title":"Datatypes","text":"<p>Throughout the DataJoint ecosystem, there are several datatypes that are used to define tables with cross-platform support (i.e. Python, MATLAB). It is important to understand these types as they can have implications in the queries you form and the capacity of their storage.</p>","location":"core/concepts/query-lang/data-types/"},{"title":"Standard Types","text":"<p>These types are largely wrappers around existing types in the current  query backend for data pipelines.</p>","location":"core/concepts/query-lang/data-types/#standard-types"},{"title":"Common Types","text":"Datatype Description Size Example Range     int integer 4 bytes <code>8</code> -231 to 231-1   enum1 category 1-2 bytes <code>M</code>, <code>F</code> -231 to 231-1   datetime2 date and time in <code>YYYY-MM-DD HH:MM:SS</code> format 5 bytes <code>'2020-01-02 03:04:05'</code>    varchar(N) string of length M, up to N M + 1-2 bytes <code>text</code>    float3 floating point number 4 bytes <code>2.04</code> 3.40E+38 to -1.17E-38, 0, and 1.17E-38 to 3.40E+38   longblob4 arbitrary numeric data \u2264 4 GiB","location":"core/concepts/query-lang/data-types/#common-types"},{"title":"Less Common Types","text":"<p>The following types add more specificity to the options above. Note that any integer type can be unsigned, shifting their range from the listed \u00b12n to from 0 - 2n+1. Float and decimal types can be similarly unsigned</p>    Datatype Description Size Example Range     tinyint tiny integer 1 byte <code>2</code> -27 to 27-1   smallint small integer 2 bytes <code>21,000</code> -215 to 215-1   mediumint medium integer 3 bytes <code>401,000</code> -223 to 223-1   date date 5 bytes <code>'2020-01-02'</code>    time time 5 bytes <code>'03:04:05'</code>    datetime5 date and time 5 bytes <code>'2020-01-02 03:04:05'</code>    char(N) string of exactly length N N bytes <code>text</code>    double double-precision floating point number 8 bytes     decimal(N,F) a fixed-point number with N total and F fractional digits 4 bytes per 9 digits     tinyblob4 arbitrary numeric data \u2272 256 bytes     blob4 arbitrary numeric data \u2264 64 KiB     mediumblob4 arbitrary numeric data \u2264 16 MiB","location":"core/concepts/query-lang/data-types/#less-common-types"},{"title":"Unique Types","text":"Datatype Description Size Example     uuid a unique GUID value 16 bytes <code>6ed5ed09-e69c-466f-8d06-a5afbf273e61</code>   attach file attachment     filepath path to external file","location":"core/concepts/query-lang/data-types/#unique-types"},{"title":"Unsupported Datatypes (for now)","text":"<ul> <li>binary</li> <li>text</li> <li>longtext</li> <li>bit</li> </ul> <p>For more information about datatypes, see  additional documentation</p>   <ol> <li> <p>enum datatypes can be useful to standardize spelling with limited categories, but use with caution. enum should not be included in primary keys, as specified values cannot be changed later.\u00a0\u21a9</p> </li> <li> <p>The default datetime value may be set to <code>CURRENT_TIMESTAMP</code>.\u00a0\u21a9</p> </li> <li> <p>Because equality comparisons are error-prone, neither float nor double should be used in primary keys. For these cases, consider decimal.\u00a0\u21a9</p> </li> <li> <p>Numeric arrays (e.g. matrix, image, structure) are compatible between MATLAB and Python(NumPy). The longblob and other blob datatypes can be configured to store data externally by using the <code>blob@store</code> syntax. For more information on storage limits see this article \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Unlike datetime, a timestamp value will be adjusted to the local time zone.\u00a0\u21a9</p> </li> </ol>","location":"core/concepts/query-lang/data-types/#unsupported-datatypes-for-now"},{"title":"Normalization","text":"<p>Coming soon!</p>","location":"core/concepts/query-lang/normalization/"},{"title":"Operators","text":"<p>Data queries make use of operators to derive the desired table. They represent the desired data symbolically, but do not contain any data. Once a query is formed, we can fetch the data into the local workspace. Since the expressions are only symbolic, repeated <code>fetch</code> calls may yield different results as the state of the database is modified.</p> <p>DataJoint implements a complete algebra of operators on tables:</p>    operator notation meaning     join A * B All matching information from A and B   restriction A &amp; cond The subset of entities from A that meet the condition   restriction A - cond The subset of entities from A that do not meet the condition   proj A.proj(...) Selects and renames attributes from A or computes new attributes   aggr A.aggr(B, ...) Same as projection with computations based on matching information in B   union A + B All unique entities from both A and B   universal set* dj.U() All unique entities from both A and B    <p>*While not technically a query operator, it is useful to discuss Universal Set in the  same context.</p>  Notes on relational algebra <p>DataJoint's algebra improves upon the classical relational algebra and upon other query languages to simplify and enhance the construction and interpretation of precise and efficient data queries.</p> <ol> <li> <p>Entity integrity: Data are represented and manipulated in the form of tables representing well-formed entity sets. This applies to the inputs and outputs of query operators. The output of a query operator is an entity set with a well-defined entity type, a primary key, unique attribute names, etc.</p> </li> <li> <p>Algebraic closure: All operators operate on entity sets and yield entity sets. Thus query expressions may be used as operands in other expressions or may be assigned to variables to be used in other expressions.</p> </li> <li> <p>Attributes are identified by names: All attributes have explicit names. This includes results of queries. Operators use attribute names to determine how to perform the operation. The order of the attributes is not significant.</p> </li> </ol>  <p>These operators are based on the concept of matching entities. Two entities match when they have no shared fields, or when their shared fields contain the same values. Any shared fields should have compatible datatypes to allow equality comparisons. Matching entities can be merged into a single entity without any conflicts of attribute names and values.</p> <p>In order for these operators to be applied to tables, they must also be  join-compatible, which means that:</p> <ol> <li> <p>All fields in both tables must be part of either the  primary key or a foreign key.</p> </li> <li> <p>All common fields must be of a compatible datatype for equality comparisons.</p> </li> </ol>  Why join compatibility restrictions? <p>These restrictions are introduced both for performance reasons and for conceptual reasons. For performance, they encourage queries that rely on indexes. For conceptual reasons, they encourage database design in which entities in different tables are related to each other by the use of primary keys and foreign keys.</p>","location":"core/concepts/query-lang/operators/"},{"title":"Join","text":"<p>The Join operator <code>A * B</code> combines the matching information in <code>A</code> and <code>B</code>. The result contains all matching combinations of entities from both arguments, including all unique primary keys from both arguments. </p> <p>In the example below, we look at the union of (A) a table pairing sessions with users and (B) a table pairing sessions with scan. </p>  <p></p>  <p>This has all the primary keys of both tables (a union thereof, shown in bold) as well as all secondary attributes (i.e., user and duration). This also excludes the session for which we don't have a scan.</p> <p>We can also join based on secondary attributes, as shown in the example below.</p>  <p></p>   Additional join properties <p>When the operands have no common attributes, the result is the cross product -- all combinations of entities. In all cases, however ...</p> <ol> <li>When <code>A</code> and <code>B</code> have the same attributes, the join <code>A * B</code> becomes equivalent to the set intersection <code>A</code> \u2229 <code>B</code>. Hence, DataJoint does not need a separate intersection operator.</li> <li>Commutativity: <code>A * B</code> is equivalent to <code>B * A</code>.</li> <li>Associativity: <code>(A * B) * C</code> is equivalent to <code>A * (B * C)</code>.</li> </ol>","location":"core/concepts/query-lang/operators/#join"},{"title":"Restriction","text":"<p>The restriction operator <code>A &amp; cond</code> selects the subset of entities from <code>A</code> that meet the condition <code>cond</code>. The exclusion operator <code>A - cond</code> selects the complement of restriction, i.e. the subset of entities from <code>A</code> that do not meet the condition <code>cond</code>. This means that the restriction and exclusion operators are complementary. The same query could be constructed using either <code>A &amp; cond</code> or <code>A - Not(cond)</code>.</p>  <p></p>  <p>The condition <code>cond</code> may be one of the following:</p> PythonMatlab   <ul> <li>another table</li> <li>a mapping, e.g. <code>dict</code></li> <li>an expression in a character string</li> <li>a collection of conditions as a <code>list</code>, <code>tuple</code>, or Pandas <code>DataFrame</code></li> <li>a Boolean expression (<code>True</code> or <code>False</code>)</li> <li>an <code>AndList</code></li> <li>a <code>Not</code> object</li> <li>a query expression</li> </ul>   <ul> <li>another table</li> <li>a mapping, or <code>struct</code></li> <li>an expression in a character string</li> <li>a collection of conditions as a <code>struct</code> or cell array</li> <li>a Boolean expression (<code>true</code> or <code>false</code>)</li> <li>a query expression</li> </ul>    <p>For more examples on each of these in Python and Matlab, please visit the documentation for the respective API.</p>  Permissive Operators <p>To circumvent compatibility checks, DataJoint offers permissive operators for  Restriction (<code>^</code>) and Join (<code>@</code>). Use with Caution.</p>","location":"core/concepts/query-lang/operators/#restriction"},{"title":"Proj","text":"<p>The <code>proj</code> operator represents projection and is used to select attributes (columns) from a table, to rename them, or to create new calculated attributes.</p> <ol> <li> <p>A simple projection selects a subset of attributes of the original table, which may not include the primary key.</p> </li> <li> <p>A more complex projection renames an attribute in another table. This could be useful when one table should be referenced multiple times in another. A user table, could contain all personnel. A project table references one person for the lead and another the coordinator, both referencing the common personnel pool.</p> </li> <li> <p>Projection can also perform calculations (as available in  MySQL) on a single attribute.</p> </li> </ol> <p>For examples of each of these in Python and Matlab, please visit the documentation for the respective API.</p>","location":"core/concepts/query-lang/operators/#proj"},{"title":"Aggr","text":"<p>Aggregation is a special form of <code>proj</code> with the added feature of allowing   aggregation calculations on another table. It has the form <code>table.aggr   (other, ...)</code> where <code>other</code> is another table. Aggregation allows adding calculated   attributes to each entity in <code>table</code> based on aggregation functions over attributes   in the matching entities of <code>other</code>.</p> <p>Aggregation functions include <code>count</code>, <code>sum</code>, <code>min</code>, <code>max</code>, <code>avg</code>, <code>std</code>, <code>variance</code>, and others. For examples in Python and Matlab, please visit the documentation for the respective API.</p>","location":"core/concepts/query-lang/operators/#aggr"},{"title":"Union","text":"<p>The result of the union operator <code>A + B</code> contains all the entities from both operands. </p> <p>Entity normalization requires that <code>A</code> and <code>B</code> are of the same type, with with the same primary key, using homologous attributes. Without secondary attributes, the result is the simple set union. With secondary attributes, they must have the same names and datatypes. The two operands must also be disjoint, without any duplicate primary key values across both inputs. These requirements prevent ambiguity of attribute values and preserve entity identity.</p>  Principles of union <ol> <li> <p>As in all operators, the order of the attributes in the operands is not significant.</p> </li> <li> <p>Operands <code>A</code> and <code>B</code> must have the same primary key attributes. Otherwise, an error will be raised.</p> </li> <li> <p>Operands <code>A</code> and <code>B</code> may not have any common non-key attributes. Otherwise, an error will be raised.</p> </li> <li> <p>The result <code>A + B</code> will have the same primary key as <code>A</code> and <code>B</code>.</p> </li> <li> <p>The result <code>A + B</code> will have all the non-key attributes from both <code>A</code> and <code>B</code>.</p> </li> <li> <p>For entities that are found in both <code>A</code> and <code>B</code> (based on the primary key), the secondary attributes will be filled from the corresponding entities in <code>A</code> and <code>B</code>.</p> </li> <li> <p>For entities that are only found in either <code>A</code> or <code>B</code>, the other operand's secondary attributes will filled with null values.</p> </li> </ol>  <p>For union, order does not matter.</p>  <p></p>   <p></p>   Properties of union <ol> <li>Commutative: <code>A + B</code> is equivalent to <code>B + A</code>.</li> <li>Associative: <code>(A + B) + C</code> is equivalent to <code>A + (B + C)</code>.</li> </ol>","location":"core/concepts/query-lang/operators/#union"},{"title":"Universal Set","text":"<p>All of the above operators are designed to preserve their input type. Some queries may require creating a new entity type not already represented by existing tables. This means that the new type must be defined as part of the query. </p> <p>Universal sets fulfill this role using <code>dj.U</code> notation. They denote the set of all possible entities with given attributes of any possible datatype. Attributes of universal sets are allowed to be matched to any namesake attributes, even those that do not come from the same initial source.</p> <p>Universal sets should be used sparingly when no suitable base tables already exist. In some cases, defining a new base table can make queries clearer and more semantically constrained.</p> <p>For examples in Python and Matlab, please visit the documentation for the respective API.</p>","location":"core/concepts/query-lang/operators/#universal-set"},{"title":"Primary Keys","text":"<p>Coming soon!</p>","location":"core/concepts/query-lang/primary-key/"},{"title":"Query Objects","text":"<p>Data queries retrieve data from the database. A data query is performed with the   help of a query object, which is a symbolic representation of the query that does   not in itself contain any actual data. The simplest query object is an instance of   a table class, representing the contents of an entire table.</p>","location":"core/concepts/query-lang/query-objs/"},{"title":"Querying a database","text":"<p>For example, if given a <code>Session</code> table, you can create a query object to retrieve its entire contents as follows:</p> PythonMatlab   <pre><code>query  = Session()\n</code></pre>   <pre><code>query = Session;\n</code></pre>    <p>More generally, a query object may be formed as a query expression constructed by applying operators to other query objects.</p> <p>For example, the following query retrieves information about all experiments and scans for mouse 001:</p> PythonMatlab   <pre><code>query = Session * Scan &amp; 'animal_id = 001'\n</code></pre> <p>Note that for brevity, query operators can be applied directly to class, as <code>Session</code> instead of <code>Session()</code>.</p>   <pre><code>query = Session * Scan &amp; 'animal_id = 001';\n</code></pre>    <p>Alternatively, we could query all scans with a sample rate over 1000, and preview the contents of the query simply displaying the object. </p> PythonMatlab   <pre><code>Scan &amp; 'sample_rate &gt; 1000'\n</code></pre>   <pre><code>Session * Scan &amp; 'sample_rate &gt; 1000';\n</code></pre>    <p>The above command shows the following table:</p> <pre><code>```text\n| id* |    start_time*      | sample_rate | signal |  times | duration |\n|-----|---------------------|-------------|--------|--------|----------| \n|  1  | 2020-01-02 22:15:00 |   1893.00   | =BLOB= | =BLOB= |  1981.29 |\n|  2  | 2020-01-03 00:15:00 |   4800.00   | =BLOB= | =BLOB= |   548.0  |\n|  3  | 2020-01-19 14:03:03 |   4800.00   | =BLOB= | =BLOB= |   336.0  |\n|  4  | 2020-01-19 14:13:03 |   4800.00   | =BLOB= | =BLOB= |  2501.0  |\n|  5  | 2020-01-23 11:05:23 |   4800.00   | =BLOB= | =BLOB= |  1800.0  |\n|  6  | 2020-01-27 14:03:03 |   4800.00   | =BLOB= | =BLOB= |   600.0  |\n|  7  | 2020-01-31 20:15:00 |   4800.00   | =BLOB= | =BLOB= |   600.0  |\n...\n11 tuples\n```\n</code></pre> <p>Note that this preview (a) only lists a few of the entities that will be returned and  (b) does not contain any data for attributes of datatype <code>blob</code>.</p> <p>Once the desired query object is formed, the query can be executed using its [fetch] (./fetch) methods. To fetch means to transfer the data represented by the query object from the database server into the workspace of the host language.</p> PythonMatlab   <pre><code>query = Scan &amp; 'sample_rate &gt; 1000'\ns = query.fetch()\n</code></pre> <p>Here fetching from the <code>query</code> object produces the NumPy record array <code>s</code> of the queried data.</p>   <pre><code>query = Session * Scan &amp; 'sample_rate &gt; 1000';\ns = query.fetch();\n</code></pre> <p>Here fetching from the <code>query</code> object produces the struct array <code>s</code> of the queried data.</p>","location":"core/concepts/query-lang/query-objs/#querying-a-database"},{"title":"Checking for entities","text":"<p>The preview of the query object shown above displayed only a few of the entities returned by the query but also displayed the total number of entities that would be returned. It can be useful to know the number of entities returned by a query, or even whether a query will return any entities at all, without having to fetch all the data themselves.</p> PythonMatlab   <p>The <code>bool</code> function applied to a query object evaluates to <code>True</code> if the query returns any entities and to <code>False</code> if the query result is empty.</p> <p>The <code>len</code> function applied to a query object determines the number of entities returned by the query.</p> <pre><code># number of sessions since the start of 2018.\nn = len(Session &amp; 'session_date &gt;= \"2018-01-01\"')\n</code></pre>   <p>The <code>exists</code> method applied to a query object evaluates to <code>true</code> if the query returns any entities and to <code>false</code> if the query result is empty.</p> <p>The <code>count</code> method applied to a query object determines the number of entities returned by the query.</p> <pre><code>% number of ephys sessions since the start of 2018.\nn = count(ephys.Session &amp; 'session_date &gt;= \"2018-01-01\"')\n</code></pre>","location":"core/concepts/query-lang/query-objs/#checking-for-entities"},{"title":"Normalization in queries","text":"<p>Query objects adhere to entity entity normalization. The result of a query will include the uniquely defining attributes jointly distinguish any two entities from each other. The query operators are designed to keep the result normalized even in complex query expressions.</p>","location":"core/concepts/query-lang/query-objs/#normalization-in-queries"},{"title":"Distributed Computing","text":"<p>Coming soon!</p>","location":"core/concepts/ref-integrity/distributed-computing/"},{"title":"Query Backend","text":"<p>Currently, DataJoint pipelines use MySQL server for their query backend.</p> <p>The following are some important topics to maintain a healthy system:</p> <ul> <li>Access Control</li> <li>Optimal Server Configuration</li> <li>Maintenance Guidelines</li> </ul>","location":"core/concepts/ref-integrity/query-backend/"},{"title":"Relational Databases","text":"<p>Coming soon!</p>","location":"core/concepts/ref-integrity/relational-databases/"},{"title":"Make Method","text":"<p>For auto-populated Imported and Computed tables1, a <code>make</code> method gives exact instructions for generating the content. By making these steps explicit, we keep a careful record of data provenance and ensure reproducibility. Data should never be entered using the <code>insert</code> method directly.</p> <p>The <code>make</code> method receives one argument: the key, which represents the upstream table entries that need populating. The <code>key</code> is a <code>dict</code> or <code>struct</code> in Python and Matlab, respectively. </p> <p>A <code>make</code> function should do three things:</p> <ol> <li> <p>Fetch data from tables upstream in the pipeline using the key for restriction.</p> </li> <li> <p>Compute and add any missing attributes to the fields already in the key.</p> </li> <li> <p>Inserts the entire entity into the triggering table.</p> </li> </ol>","location":"core/concepts/reproduce/make-method/"},{"title":"Populate","text":"<p>The <code>make</code> method is sometimes referred to as the <code>populate</code> function because this is the class method called to run the <code>make</code> method on all relevant keys2.</p> PythonMatlab   <pre><code>Segmentation.populate()\n</code></pre>   <p>In Matlab, the <code>key</code> is a <code>struct</code>.  <pre><code>populate(Segmentation)\n</code></pre></p>    <p>For more information on the <code>populate</code> options in each language, please visit the  Python and Matlab documentation pages.</p>    <ol> <li> <p>For information on differentiating these data tiers, see the Table Tier section on Automation.\u00a0\u21a9</p> </li> <li> <p>For information on reprocessing keys that resulted in an error, see information on the Jobs table.\u00a0\u21a9</p> </li> </ol>","location":"core/concepts/reproduce/make-method/#populate"},{"title":"Table Tiers","text":"<p>The key to reproducibility in DataJoint is clear data provenance. In any experiment, there are stages for data entry, ingestion, and processing or analysis. DataJoint helps make these stages explicit with data tiers, indicating data origin.</p>    Table Type Description Example     Lookup Small reference tables containing general information or settings. Analysis parameter set.   Manual Data entered entered with by hand or with external helper scripts. Manual subject metadata entry.   Imported Data ingested automatically from outside files. Loading a raw data file.   Computed Data computed automatically entirely inside the pipeline. Running analyses and storing results.   Part* Data in a many-to-one relationship with the corresponding master table. Independent unit results from a given analysis.      *Part tables <p>While all other types correspond to their data tier, Part tables inherit the tier of their master table.</p>  <p>Lookup and Manual tables generally handle manually added data. Imported and Computed tables both allow for automation, but differ in the source of information. And Part tables have a unique relationship to their corresponding Master table.</p>","location":"core/concepts/reproduce/table-tiers/"},{"title":"Data Entry: Lookup and Manual","text":"<p>Manual tables are populated during experiments through a variety of interfaces. Not all manual information is entered by typing. Automated software can enter it directly into the database. What makes a manual table manual is that it does not perform any computations within the DataJoint pipeline. </p> <p>Lookup tables contain basic facts that are not specific to an experiment and are fairly persistent. In GUIs, lookup tables are often used for drop-down menus or radio buttons. In Computed tables, the contents of Lookup tables are often used to specify alternative methods for computations. Unlike Manual tables, Lookup tables can specify contents in the schema definition. For syntax, please visit the  Python and  Matlab documentation pages.</p> <p>Lookup tables are especially useful for entities with many unique features. Rather than adding many primary keys, this information can be retrieved through an index. For an example, see ClusteringParamSet in Element Array Ephys.</p>  <p>While this distinction is useful for structuring a pipeline, it is not enforced, and left to the best judgement of the researcher.</p>","location":"core/concepts/reproduce/table-tiers/#data-entry-lookup-and-manual"},{"title":"Automation: Imported and Computed","text":"<p>Auto-populated tables are used to define, execute, and coordinate computations in a DataJoint pipeline. These tables belong to one of the two auto-populated data tiers: Imported and Computed. The difference is not strictly enforced, but the convention helps researchers understand data provenance at a glance.</p> <p>Imported tables require access to external files, such as raw storage, outside the  database. If a entry were deleted, it could be retrieved from the raw files on disk.  An EphysRecording table, for example, would load metadata and raw data from  experimental recordings. </p>  <p>Computed tables only require to other data within the pipeline. If an entry were  deleted, it could could be recovered by simply running the relevant command. For   analysis, many pipelines feature a task table that pairs sets of primary keys ready  for computation. The  PoseEstimationTask  in Element DeepLabCut pairs videos and models. The   PoseEstimation  table executes these computations and stores the results.</p> <p>Data should never be directly inserted into auto-populated tables. Instead, these tables specify a <code>make</code> method. </p>","location":"core/concepts/reproduce/table-tiers/#automation-imported-and-computed"},{"title":"Master-Part Relationship","text":"<p>An entity in one table might be inseparably associated with a group of entities in another, forming a master-part relationship, with two important features.</p> <ol> <li> <p>Part tables permit a many-to-one relationship with the master. </p> </li> <li> <p>Data entry and deletion should impact all part tables as well as the master. </p> </li> </ol> <p>If you're considering adding a Part table, consider whether or not there could be a reason to modify the part but not the master. If so, Manual and/or Lookup tables are likely more appropriate. Populate and delete commands should always target the master, and never individual parts. This facilitates data integrity by treating the entire process as one transaction. Either (a) all data are inserted/committed or deleted, or (b) the entire transaction is rolled back. This ensures that partial results never appear in the database.</p> <p>As an example, Element Calcium Imaging features a MotionCorrection computed table segmenting an image into masks. The resulting correctoion is inseparable from the rigid and nonrigid correction parameters that it produces, with MotionCorrection.RigidMotionCorrection and MotionCorrection.NonRigidMotionCorrection  part tables. </p>  <p>The master-part relationship cannot be chained or nested. DataJoint does not allow part tables of other part tables. However, it is common to have a master table with multiple part tables that depend on each other. See link above.</p>","location":"core/concepts/reproduce/table-tiers/#master-part-relationship"},{"title":"Example","text":"<pre><code>  flowchart TB\n    Mouse --&gt; \n    Session ==&gt; Scan %% Thick line\n    Scan --&gt; Alignment((Alignment)) --&gt; Segmentation --&gt; Trace((Trace)) --&gt; RF((RF))\n    Scan --&gt; Stimulus --&gt; RF --&gt; Field\n    SegmentationMethod -.-&gt; Segmentation((Segmentation)) %% Circle shape\n    %% class list,of,nodes class-name\n    class Alignment,Mouse,RF,Scan,Segmentation,SegmentationMethod,Session,Stimulus,Trace,Field all;\n    class Mouse,Scan,Session,Stimulus manual;\n    class Segmentation,Trace,RF compute;\n    class SegmentationMethod lookup;\n    class Alignment import;\n    class Field part;\n    %% classDef class-name option1:var1,option2:var2;\n    classDef all stroke:#333;   %% Grey stroke around all tables\n    classDef manual fill:#060;  %% Green manual tables\n    classDef compute fill:#600; %% Red compute tables\n    classDef import fill:#006;  %% Blue import tables\n    classDef lookup fill:#ddd;  %% Grey lookup tables\n    classDef part fill:#FFF;    %% White part tables\n    %% Above colors were chosen to be compatible with #00a0df as text</code></pre> <p>In this example, the experimenter first enters information into the Manual tables, shown in green. They enter information about a mouse, then a session, and then each scan performed, with the stimuli. Next the automated portion of the pipeline takes over, Importing the raw data and performing image alignment, shown in blue. Computed tables are shown in red. Image segmentation identifies cells in the images, and extraction of calcium traces. In grey, the segmentation method is a Lookup table. Finally, the receptive field (RF) computation is performed by relating the imaging signals to the visual stimulus information.</p> <p>For more information on table dependencies and diagrams, see their respective articles:</p> <ul> <li>Dependencies</li> <li>Diagrams</li> </ul>","location":"core/concepts/reproduce/table-tiers/#example"},{"title":"DataJoint Elements for Neurophysiology","text":"<p>DataJoint Elements provides an efficient approach for neuroscience labs to create and manage scientific data workflows: the complex multi-step methods for data collection, preparation, processing, analysis, and modeling that researchers must perform in the course of an experimental study. Elements are a collection of curated modules for assembling workflows for several modalities of neurophysiology experiments and are designed for ease of integration into diverse custom workflows. This work is derived from the developments in leading neuroscience projects and uses the DataJoint API for defining, deploying, and sharing their data workflows.</p> <p>An overview of the principles of DataJoint workflows and the goals of DataJoint Elements are described in the position paper \"DataJoint Elements: Data Workflows for Neurophysiology\".</p> <p>Below are the projects that make up the family of open-source DataJoint Elements:</p>  <ul> <li> Element Calcium Imaging <p>A data pipeline for calcium imaging microscopy.</p> <p> Learn more</p> </li> </ul> <ul> <li> Element Array Electrophysiology <p>A data pipeline for Neuropixels probes.</p> <p> Learn more</p> </li> </ul> <ul> <li> Element Electrode Localization <p>A data pipeline for electrode localization of Neuropixels probes.</p> <p> Learn more</p> </li> </ul> <ul> <li> Element Miniscope <p>A data pipeline for miniscope calcium imaging.</p> <p> Learn more</p> </li> </ul> <ul> <li> Element DeepLabCut <p>A data pipeline for pose estimation with DeepLabCut.</p> <p> Learn more</p> </li> </ul> <ul> <li> Element Facemap <p>A data pipeline for pose estimation with Facemap.</p> <p> New docs coming soon!</p> </li> </ul> <ul> <li> Element Visual Stimulus <p>A data pipeline for visual stimulation with Psychtoolbox.</p> <p> New docs coming soon!</p> </li> </ul> <ul> <li> Element Lab <p>A data pipeline for lab management.</p> <p> Learn more</p> </li> </ul> <ul> <li> Element Animal <p>A data pipeline for subject management.</p> <p> Learn more</p> </li> </ul> <ul> <li> Element Session <p>A data pipeline for session management.</p> <p> Learn more</p> </li> </ul> <ul> <li> Element Event <p>A data pipeline for event- and trial-based experiments.</p> <p> Learn more</p> </li> </ul> <ul> <li> Element Interface <p>Common functions for the DataJoint Elements.</p> <p> New docs coming soon!</p> </li> </ul>","location":"elements/"},{"title":"Concepts","text":"<p>The following conventions describe the DataJoint Python API implementation. </p>","location":"elements/concepts/"},{"title":"DataJoint Schemas","text":"<p>The DataJoint Python API allows creating database schemas, which are namespaces for collections of related tables.</p> <p>The following commands declare a new schema and create the object named <code>schema</code> to reference the database schema.</p> PythonMatlab   <pre><code>import datajoint as dj\nschema = dj.schema('&lt;schema_name&gt;')\n</code></pre> <p>We follow the convention of having only one schema defined per Python module. Then such a module becomes a DataJoint schema comprising a Python module with a corresponding database schema.</p> <p>The module's <code>schema</code> object is then used as the decorator for classes that define tables in the database. </p>   <pre><code>dj.createSchema\n</code></pre> <p>In Matlab, we list one table per file and place schemas in folders.</p>","location":"elements/concepts/#datajoint-schemas"},{"title":"Elements","text":"<p>An Element is a software package defining one or more DataJoint schemas serving a particular purpose. By convention, such packages are hosted in individual GitHub repositories. For example, Element <code>element_calcium_imaging</code> is hosted at https://github.com/datajoint/element-calcium-imaging and contains two DataJoint schemas: <code>scan</code> and <code>imaging</code>.</p>","location":"elements/concepts/#elements"},{"title":"YouTube Tutorials","text":"<p>The following YouTube videos provide information on basic design principles and file organization.</p> <ul> <li>Why neuroscientists should use relational databases   compared to traditional file heirarchies.</li> <li>Quickstart Guide including    terminology, and how to read DataJoint Diagrams and DataJoint Python table    definitions.</li> <li>Intro to the Element and Workflow files   for an overview of the respective GitHub repositories.</li> <li>Overview of upstream Elements to    ingest and explore Lab, Animal, and Session metadata. </li> </ul>  Note <p>Some videos feature outdated versions of the respective GitHub repositories. For the most updated information, check the documentation page for the corresponding Element.</p>","location":"elements/concepts/#youtube-tutorials"},{"title":"Deferred schemas","text":"<p>A deferred schema is one in which the name of the database schema name is not specified. This module does not declare schema and tables upon import. Instead, they are declared by calling <code>schema.activate('&lt;schema_name&gt;')</code> after import.</p> <p>By convention, all modules corresponding to deferred schema must declare the function <code>activate</code> which in turn calls <code>schema.activate</code>.</p> <p>Thus, Element modules begin with:</p> <pre><code>import datajoint as dj\nschema = dj.schema()\n\ndef activate(schema_name):\nschema.activate(schema_name)\n</code></pre> <p>However, many activate functions perform other work associated with activating the schema such as activating other schemas upstream.</p>","location":"elements/concepts/#deferred-schemas"},{"title":"Linking Module","text":"<p>To make the code more modular with fewer dependencies, Element modules do not <code>import</code> upstream schemas directly. Instead, all required classes and functions must be defined in a <code>linking_module</code> and passed to the module's <code>activate</code> function. By keeping all upstream requirements in the linking module, all Elements can be activated as part of any larger pipeline.</p> <p>For instance, the  Scan module receives its required functions from the linking module passed into the module's <code>activate</code> function. See the  corresponding workflow for an example of how the linking module is passed into the Element's module.</p>","location":"elements/concepts/#linking-module"},{"title":"Developer instructions","text":"","location":"elements/developer-guide/"},{"title":"Development mode installation","text":"<ul> <li>We recommend doing development work in a conda environment. For information on setting   up conda for the first time, see    this article.</li> </ul> <ul> <li>This method allows you to modify the source code for example DataJoint   workflows (e.g. <code>workflow-array-ephys</code>) and their   dependencies (e.g., <code>element-array-ephys</code>).</li> </ul> <ul> <li>Launch a new terminal and change directory to where you want to clone the   repositories (e.g., <code>bash cd ~/Projects</code>)</li> </ul> <ul> <li>Clone the relevant workflow and refer to the <code>requirements.txt</code> in the workflow for   the list of Elements to clone and install as editable. You will also need to install   <code>element-interface</code> <pre><code>deps=(\"lab\" \"animal\" \"session\" \"interface\" \"&lt;others&gt;\")\nfor repo in $deps # clone each\ndo \n    git clone https://github.com/datajoint/element-$repo\ndone\nfor repo in $(ls -d ./{element,workflow}*) # editable install \ndo \n    pip install -e ./$repo\ndone\n</code></pre> </li> </ul>","location":"elements/developer-guide/#development-mode-installation"},{"title":"Drop schemas","text":"<p>If you need to drop all schemas to start fresh, you'll need to do following the dependency order. Refer to the workflow's notebook (<code>notebooks/06-drop-optional.ipynb</code>) for the drop order.</p>","location":"elements/developer-guide/#drop-schemas"},{"title":"Pytests","text":"<ul> <li>Download the test dataset to your local machine. Note the directory where the dataset   is saved (e.g. <code>/tmp/testset</code>).</li> </ul> <ul> <li>Create an <code>.env</code> file within the <code>docker</code> directory with the following content.   Replace <code>/tmp/testset</code> with the directory where you have the test dataset downloaded.   <code>TEST_DATA_DIR=/tmp/testset</code></li> </ul> <ul> <li>If testing an unreleased version of the <code>element</code> or your fork of an <code>element</code> or the   <code>workflow</code>, within the <code>Dockerfile</code> uncomment the lines from the different options   presented. This will allow you to install the repositories of interest and run the   integration tests on those packages. Be sure that the <code>element</code> package version   matches the version in the <code>requirements.txt</code> of the <code>workflow</code>.</li> </ul> <ul> <li>Run the Docker container.   <pre><code>docker-compose -f ./docker/docker-compose-test.yaml up --build\n</code></pre></li> </ul>","location":"elements/developer-guide/#pytests"},{"title":"Jupytext","text":"<p>We maintain <code>.py</code> script copies of all didactic notebooks to facilitate the GitHub review process. From the main workflow directory, we recommend the following command to generate these copies. You may wish to save this as an alias in your <code>.bash_profile</code>. Note that the jupytext sync features may cause issues with the original notebooks.</p> <pre><code>pip install jupytext\njupytext --to py notebooks/0*ipynb; mv notebooks/*py notebooks/py_scripts\n</code></pre>","location":"elements/developer-guide/#jupytext"},{"title":"Auxiliary tools","text":"<ul> <li>DataJoint utilities - general-purpose functions/utilities for working with DataJoint pipelines<ul> <li>Text search</li> <li>Data migration between tables/schemas/servers</li> <li>DataJoint workers</li> </ul> </li> <li>DataJoint for Julia - wrapper library developed in Carlos Brody lab</li> </ul>","location":"elements/developer-guide/#auxiliary-tools"},{"title":"User setup instructions","text":"<p>The following document describes how to setup a development environment and connect to a database so that you can use the DataJoint Elements to build and run a workflow on  your local machine. </p> <p>If you'd rather run through similar tutorials in an online  Jupyter environment, please visit CodeBook.</p> <p>Any of the DataJoint Elements can be combined together to create a workflow that matches your experimental setup. We have a number of example workflows to get you started. Each focuses on a specific modality, but they can be adapted for your custom workflow. </p> <ol> <li> <p>Getting up and running will require a couple items for a good development   environment. If any of these items are already familiar to   you and installed on your machine, you can skip the corresponding section.</p> <p>1. Python</p> <p>2. Conda</p> <p>3. Integrated Development Environment</p> <p>4. Version Control (git)</p> <p>5. Visualization packages</p> </li> <li> <p>Next, you'll need to download one of the example workflows and    corresponding example data. </p> </li> <li> <p>Finally, there are a couple different approaches to   connecting to a database. Here, we highlight three approaches:</p> <p>1. First Time: Beginner. Temporary storage to learn the ropes.</p> <p>2. Local Database: Intermediate. Deployed on local hardware, managed        by you.</p> <p>3. Central Database: Advanced: Deployed on dedicated hardware.</p> </li> </ol>","location":"elements/user-guide/"},{"title":"Development Environment","text":"<p>This diagram describes the general components for a local DataJoint environment. </p> <pre><code>flowchart LR\n  py_interp  --&gt;|DataJoint| db_server[(\"Database Server\\n(e.g., MySQL)\")]\n  subgraph conda[\"Conda environment\"]\n    direction TB\n    py_interp[Python Interpreter]\n  end\n  subgraph empty1[\" \"] %% Empty subgraphs prevent overlapping titles\n    direction TB\n    style empty1 fill:none, stroke-dasharray: 0 1\n    conda\n  end\n  subgraph term[\"Terminal or Jupyter Notebook\"]\n    direction TB\n    empty1\n  end\n  subgraph empty2[\" \"] %% Empty subgraphs prevent overlapping titles\n    direction TB\n    style empty2 fill:none, stroke-dasharray: 0 1\n    term\n  end\n  subgraph ide[\"Integrated Development Environment\"]\n    direction TB\n    empty2\n  end\n  class py_interp,conda,term,ide,db_server,DataJoint boxes;\n  classDef boxes fill:#ddd, stroke:#333;</code></pre>","location":"elements/user-guide/#development-environment"},{"title":"Python","text":"<p>DataJoint Elements are written in Python. The DataJoint Python API supports Python  versions 3.7 and up. We recommend downloading the latest stable release of 3.9 here, and following the install instructions. </p>","location":"elements/user-guide/#python"},{"title":"Conda","text":"<p>Python projects each rely on different depenencies, which may conflict across projects. We recommend working in a Conda environment for each project to isolate the dependencies. For more information on why Conda, and setting up the version of Conda that best suits your needs, see  this article.</p> <p>To get going quickly, we recommend you ...</p> <ol> <li> <p>Download Miniconda and go through the setup, including adding Miniconda to your <code>PATH</code> (full   instructions    here).</p> </li> <li> <p>Declare and initialize a new conda environment with the following commands. Edit    <code>&lt;name&gt;</code> to reflect your project.</p> <pre><code>conda create --name datajoint-workflow-&lt;name&gt; python=3.9 \nconda activate dj-workflow-&lt;name&gt; \n</code></pre> </li> </ol>  Apple M1 users: Click to expand <p>Running analyses with Element DeepLabCut or Element Calcium imaging may require tensorflow, which can cause issues on M1 machines. By saving the <code>yaml</code>  file below, this environment can be loaded with <code>conda create -f my-file.yaml </code>. If you encounter errors related to <code>clang</code>, try launching xcode  and retrying.</p> <pre><code>name: dj-workflow-&lt;name&gt;\nchannels:\n    - apple \n    - conda-forge\n    - defaults\ndependencies:\n    - tensorflow-deps\n    - opencv\n    - python=3.9\n    - pip&gt;=19.0 \n    - pip:\n        - tensorflow-macos\n        - tensorflow-metal\n        - datajoint\n</code></pre>","location":"elements/user-guide/#conda"},{"title":"Integrated Development Environment (IDE)","text":"<p>Development and use can be done with a plain text editor in the terminal. However, an integrated development environment (IDE) can improve your experience. Several IDEs are available. We recommend  Microsoft's Visual Studio Code, also called  VS Code. To set up VS Code with Python for the first time, follow  this tutorial.</p>","location":"elements/user-guide/#integrated-development-environment-ide"},{"title":"Version Control (git)","text":"<p>Table definitions and analysis code can change over time, especially with multiple collaborators working on the same project. Git is an open-source, distributed version control system that helps keep track of what changes where made when, and by whom. GitHub is a platform that hosts projects managed with git. The example DataJoint Workflows are hosted on GitHub, we will use git to clone (i.e., download) this repository.</p> <ol> <li>Check if you already have git by typing <code>git --version</code> in a terminal window.</li> <li>If git is not installed on your system, please install git. </li> <li>You can read more about git basics here.</li> </ol>","location":"elements/user-guide/#version-control-git"},{"title":"Visualization packages (Jupyter Notebooks, DataJoint Diagrams)","text":"<p>To run the demo notebooks and generate visualizations associated with an example workflow, you'll need a couple extra packages. </p> <p>Jupyter Notebooks help structure code (see here for full instructions on Jupyter within VS Code).</p> <ol> <li> <p>Install Jupyter packages     <pre><code>conda install jupyter ipykernel nb_conda_kernels\n</code></pre></p> </li> <li> <p>Ensure your VS Code python intepreter is set to your Conda environment path.</p> <p>  Click to expand more details. <ul> <li>View &gt; Command Palette</li> <li>Type \"Python: Select Interpreter\", hit enter.</li> <li>If asked, select the workspace where you plan to download the workflow.</li> <li>If present, select your Conda environment. If not present, enter in the        path.</li> </ul> </p> </li> </ol> <p>DataJoint Diagrams rely on additional packages. To install these packages, enter the following command...     <pre><code>conda install graphviz python-graphviz pydotplus\n</code></pre></p>","location":"elements/user-guide/#visualization-packages-jupyter-notebooks-datajoint-diagrams"},{"title":"Example Config, Workflows and Data","text":"<p>Of the options below, pick the workflow that best matches your  needs.</p> <ol> <li> <p>Change the directory to where you want to download the workflow.</p> <pre><code>cd ~/Projects\n</code></pre> </li> <li> <p>Clone the relevant repository, and change directories to this new directory.     <pre><code>git clone https://github.com/datajoint/&lt;repository&gt;\ncd &lt;repository&gt;\n</code></pre></p> </li> <li> <p>Install this directory as editable with the <code>-e</code> flag.     <pre><code>pip install -e .\n</code></pre>  Why editable? Click for details         This lets you modify the code after installation and experiment with different         designs or adding additional tables. You may wish to edit <code>pipeline.py</code> or          <code>paths.py</code> to better suit your needs. If no modification is required,          using <code>pip install .</code> is sufficient.     </p> </li> <li> <p>Install <code>element-interface</code>, which has utilities used across different Elements and     Workflows.</p> <pre><code>pip install \"element-interface @ git+https://github.com/datajoint/element-interface\"\n</code></pre> </li> <li> <p>\u200bSet up a local DataJoint config file by saving the      following block as a json in your workflow directory as <code>dj_local_conf.json</code>. Not     sure what to put for the <code>&lt; &gt;</code> values below? We'll cover this when we      connect to the database</p> <pre><code>{\n    \"database.host\": \"&lt;hostname&gt;\",\n    \"database.user\": \"&lt;username&gt;\",\n    \"database.password\": \"&lt;password&gt;\",\n    \"loglevel\": \"INFO\",\n    \"safemode\": true,\n    \"display.limit\": 7,\n    \"display.width\": 14,\n    \"display.show_tuple_count\": true,\n    \"custom\": {\n        \"database.prefix\": \"&lt;username_&gt;\"\n    }\n}\n</code></pre> </li> </ol>","location":"elements/user-guide/#example-config-workflows-and-data"},{"title":"Example Workflows","text":"<ul> <li> Workflow Session <p>An example workflow for session management.</p> <p> Clone from GitHub</p> </li> </ul> <ul> <li> Workflow Array Electrophysiology <p>An example workflow for Neuropixels probes.</p> <p> Clone from GitHub</p> </li> </ul> <ul> <li> Workflow Calcium Imaging <p>An example workflow for calcium imaging microscopy.</p> <p> Clone from GitHub</p> </li> </ul> <ul> <li> Workflow Miniscope <p>An example workflow for miniscope calcium imaging.</p> <p> Clone from GitHub</p> </li> </ul> <ul> <li> Workflow DeepLabCut <p>An example workflow for pose estimation with DeepLabCut.</p> <p> Clone from GitHub</p> </li> </ul>","location":"elements/user-guide/#example-workflows"},{"title":"Example Data","text":"<p>The first notebook in each workflow will guide you through downloading example data from DataJoint's AWS storage archive. You can also process your own data. To use the  example data, you would ...</p> <ol> <li> <p>Install <code>djarchive-client</code></p> <pre><code>pip install git+https://github.com/datajoint/djarchive-client.git\n</code></pre> </li> <li> <p>Use a python terminal to import the <code>djarchive</code> client and view available datasets,     and revisions.</p> <pre><code>import djarchive_client\nclient = djarchive_client.client()\nlist(client.datasets())  # List available datasets, select one\nlist(client.revisions()) # List available revisions, select one\n</code></pre> </li> <li> <p>Prepare a directory to store the download data, for example in <code>/tmp</code>, then download    the data with the <code>djarchive</code> client. This may take some time with larger datasets.</p> <pre><code>import os\nos.makedirs('/tmp/example_data/', exist_ok=True)\nclient.download(\n    '&lt;workflow-dataset&gt;',\n    target_directory='/tmp/example_data',\n    revision='&lt;revision&gt;'\n)\n</code></pre> </li> </ol>","location":"elements/user-guide/#example-data"},{"title":"Example Data Organization","text":"Array Ephys: Click to expand details <ul> <li>Dataset: workflow-array-ephys-benchmark</li> <li>Revision: 0.1.0a4</li> <li>Size: 293 GB</li> </ul> <p>The example <code>subject6/session1</code> data was recorded with SpikeGLX and processed with Kilosort2.  <pre><code>/tmp/example_data/\n- subject6\n- session1\n    - towersTask_g0_imec0\n    - towersTask_g0_t0_nidq.meta\n    - towersTask_g0_t0.nidq.bin\n</code></pre> Element and Workflow Array Ephys also support data recorded with  OpenEphys.</p>   Calcium Imaging: Click to expand details <ul> <li>Dataset: workflow-array-calcium-imaging-test-set</li> <li>Revision: 0_1_0a2</li> <li>Size: 142 GB</li> </ul> <p>The example <code>subject3</code> data was recorded with Scanbox.  The example <code>subject7</code> data was recorded with ScanImage. Both datasets were processed with Suite2p. <pre><code>/tmp/example_data/\n- subject3/\n    - 210107_run00_orientation_8dir/\n        - run00_orientation_8dir_000_000.sbx\n        - run00_orientation_8dir_000_000.mat\n        - suite2p/\n            - combined\n            - plane0\n            - plane1\n            - plane2\n            - plane3\n- subject7/\n    - session1\n        - suite2p\n            - plane0\n</code></pre> Element and Workflow Calcium Imaging also support data collected with ... - Nikon - Prairie View - CaImAn</p>   DeepLabCut: Click to expand details <ul> <li>Dataset: workflow-dlc-data</li> <li>Revision: v1</li> <li>Size: .3 GB</li> </ul> <p>The example data includes both training data and pretrained models. <pre><code>/tmp/test_data/from_top_tracking/\n- config.yml\n- dlc-models/iteration-0/from_top_trackingFeb23-trainset95shuffle1/\n    - test/pose_cfg.yaml\n    - train/\n        - checkpoint\n        - checkpoint_orig\n        \u2500 learning_stats.csv\n        \u2500 log.txt\n        \u2500 pose_cfg.yaml\n        \u2500 snapshot-10300.data-00000-of-00001\n        \u2500 snapshot-10300.index\n        \u2500 snapshot-10300.meta   # same for 103000\n- labeled-data/\n    - train1/\n        - CollectedData_DJ.csv\n        - CollectedData_DJ.h5\n        - img00674.png          # and others\n    - train2/                   # similar to above\n- videos/\n    - test.mp4\n    - train1.mp4\n</code></pre></p>   FaceMap: Click to expand details <p>Associated workflow still under development</p> <ul> <li>Dataset: workflow-facemap</li> <li>Revision: 0.0.0</li> <li>Size: .3 GB</li> </ul>","location":"elements/user-guide/#example-data-organization"},{"title":"Using Your Own Data","text":"<p>Some of the workflows carry some assumptions about how your file directory will be  organized, and how some files are named.</p>  Array Ephys: Click to expand details <ul> <li> <p>In your DataJoint config, add another item under <code>custom</code>,      <code>ephys_root_data_dir</code>, for your local root data directory. This can include      multiple roots.</p> <pre><code>\"custom\": {\n    \"database.prefix\": \"&lt;username_&gt;\",\n    \"ephys_root_data_dir\": [\"/local/root/dir1\", \"/local/root/dir2\"]\n}\n</code></pre> </li> </ul> <ul> <li>The <code>subject</code> directory names must match the subject IDs in your subjects table.      The <code>ingest.py</code> script (     demo ingestion notebook     ) can help load these values from <code>./user_data/subjects.csv</code>.</li> </ul> <ul> <li>The <code>session</code> directories can have any naming convention, but must be specified      in the session table (see also     demo ingestion notebook     ). </li> </ul> <ul> <li>Each session can have multiple probes.</li> </ul> <ul> <li>The <code>probe</code> directory names must end in a one-digit number corresponding to the      probe number.</li> </ul> <ul> <li>Each <code>probe</code> directory should contain:     - One neuropixels meta file named <code>*[0-9].ap.meta</code>     - Optionally, one Kilosort output folder</li> </ul> <p>Folder structure: <pre><code>&lt;ephys_root_data_dir&gt;/\n\u2514\u2500\u2500\u2500&lt;subject1&gt;/                       # Subject name in `subjects.csv`\n\u2502   \u2514\u2500\u2500\u2500&lt;session0&gt;/                   # Session directory in `sessions.csv`\n\u2502   \u2502   \u2514\u2500\u2500\u2500imec0/\n\u2502   \u2502   \u2502   \u2502   *imec0.ap.meta\n\u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500ksdir/\n\u2502   \u2502   \u2502       \u2502   spike_times.npy\n\u2502   \u2502   \u2502       \u2502   templates.npy\n\u2502   \u2502   \u2502       \u2502   ...\n\u2502   \u2502   \u2514\u2500\u2500\u2500imec1/\n\u2502   \u2502       \u2502   *imec1.ap.meta\n\u2502   \u2502       \u2514\u2500\u2500\u2500ksdir/\n\u2502   \u2502           \u2502   spike_times.npy\n\u2502   \u2502           \u2502   templates.npy\n\u2502   \u2502           \u2502   ...\n\u2502   \u2514\u2500\u2500\u2500&lt;session1&gt;/\n\u2502   \u2502   \u2502   ...\n\u2514\u2500\u2500\u2500&lt;subject2&gt;/\n\u2502   \u2502   ...\n</code></pre></p>   Calcium Imaging: Click to expand details <p>Note: While Element Calcium Imaging can accommodate multiple scans per  session, Workflow Calcium Imaging assumes there is only one scan per session.</p> <ul> <li> <p>In your DataJoint config, add another item under <code>custom</code>,      <code>imaging_root_data_dir</code>, for your local root data directory. </p> <pre><code>\"custom\": {\n    \"database.prefix\": \"&lt;username_&gt;\",\n    \"imaging_root_data_dir\": \"/local/root/dir1\"\n}\n</code></pre> </li> </ul> <ul> <li>The <code>subject</code> directory names must match the subject IDs in your subjects table.      The <code>ingest.py</code> script (     demo ingestion notebook     ) can help load these values from <code>./user_data/subjects.csv</code>.</li> </ul> <ul> <li>The <code>session</code> directories can have any naming convention, but must be specified      in the session table (see also     [demo ingestion notebook])(https://github.com/datajoint/workflow-calcium-imaging/blob/main/notebooks/04-automate-optional.ipynb)     . </li> </ul> <ul> <li>Each <code>session</code> directory should contain:     - All <code>.tif</code> or <code>.sbx</code> files for the scan, with any naming convention.     - One <code>suite2p</code> subfolder, containing the analysis outputs in the default naming          convention.     - One <code>caiman</code> subfolder, containing the analysis output <code>.hdf5</code> file, with any         naming convention.</li> </ul> <p>Folder structure: <pre><code>imaging_root_data_dir/\n\u2514\u2500\u2500\u2500&lt;subject1&gt;/                     # Subject name in `subjects.csv`\n\u2502   \u2514\u2500\u2500\u2500&lt;session0&gt;/                 # Session directory in `sessions.csv`\n\u2502   \u2502   \u2502   scan_0001.tif\n\u2502   \u2502   \u2502   scan_0002.tif\n\u2502   \u2502   \u2502   scan_0003.tif\n\u2502   \u2502   \u2502   ...\n\u2502   \u2502   \u2514\u2500\u2500\u2500suite2p/\n\u2502   \u2502       \u2502   ops1.npy\n\u2502   \u2502       \u2514\u2500\u2500\u2500plane0/\n\u2502   \u2502       \u2502   \u2502   ops.npy\n\u2502   \u2502       \u2502   \u2502   spks.npy\n\u2502   \u2502       \u2502   \u2502   stat.npy\n\u2502   \u2502       \u2502   \u2502   ...\n\u2502   \u2502       \u2514\u2500\u2500\u2500plane1/\n\u2502   \u2502           \u2502   ops.npy\n\u2502   \u2502           \u2502   spks.npy\n\u2502   \u2502           \u2502   stat.npy\n\u2502   \u2502           \u2502   ...\n\u2502   \u2502   \u2514\u2500\u2500\u2500caiman/\n\u2502   \u2502       \u2502   analysis_results.hdf5\n\u2502   \u2514\u2500\u2500\u2500&lt;session1&gt;/                 # Session directory in `sessions.csv`\n\u2502   \u2502   \u2502   scan_0001.tif\n\u2502   \u2502   \u2502   scan_0002.tif\n\u2502   \u2502   \u2502   ...\n\u2514\u2500\u2500\u2500&lt;subject2&gt;/                     # Subject name in `subjects.csv`\n\u2502   \u2502   ...\n</code></pre></p>   DeepLabCut: Click to expand details <p>Note: Element DeepLabCut assumes you've already used the DeepLabCut GUI to  set up your project and label your data. This can include multiple roots.</p> <ul> <li>In your DataJoint config, add another item under      <code>custom</code>, <code>dlc_root_data_dir</code>, for your local root      data directory.     <pre><code>\"custom\": {\n    \"database.prefix\": \"&lt;username_&gt;\",\n    \"dlc_root_data_dir\": [\"/local/root/dir1\", \"/local/root/dir2\"]\n}\n</code></pre></li> </ul> <ul> <li>You have preserved the default DeepLabCut project directory, shown below.</li> </ul> <ul> <li>The paths in your various <code>yaml</code> files reflect the current folder structure.</li> </ul> <ul> <li>You have generated thge <code>pickle</code> and <code>mat</code> training files. If not, follow the      DeepLabCut guide to      create a training dataset</li> </ul> <p>Folder structure: <pre><code>/dlc_root_data_dir/your_project/\n- config.yaml                   # Including correct path information\n- dlc-models/iteration-*/your_project_date-trainset*shuffle*/\n    - test/pose_cfg.yaml        # Including correct path information\n    - train/pose_cfg.yaml       # Including correct path information\n- labeled-data/any_names/*{csv,h5,png}\n- training-datasets/iteration-*/UnaugmentedDataSet_your_project_date/\n    - your_project_*shuffle*.pickle\n    - your_project_scorer*shuffle*.mat\n- videos/any_names.mp4\n</code></pre></p>   Miniscope: Click to expand details <ul> <li> <p>In your DataJoint config, add another item under <code>custom</code>,      <code>miniscope_root_data_dir</code>, for your local root data directory.</p> <pre><code>\"custom\": {\n    \"database.prefix\": \"&lt;username_&gt;\",\n    \"miniscope_root_data_dir\": \"/local/root/dir\"\n}\n</code></pre> </li> </ul>","location":"elements/user-guide/#using-your-own-data"},{"title":"Relational databases","text":"<p>DataJoint helps you connect to a database server from your programming environment  (i.e., Python or MATLAB), granting a number of benefits over traditional file heirarchies  (see YouTube Explainer). We offer two  options:</p> <ol> <li>The First Time beginner approach loads example data to a temporary existing     database, saving you setup time. But, because this data will be purged intermittently,    it should not be used in a true experiment.</li> <li>The Local Database intermediate approach will walk you through     setting up your own database on your own hardware. While easier to manage, it may be     difficult to expose this to outside collaborators.</li> <li>The Central Database advanced approach has the benefits of running on dedicated hardware, but may require significant IT expertise and infrastructure depending on your needs.</li> </ol>","location":"elements/user-guide/#relational-databases"},{"title":"First time","text":"<p>Temporary storage. Not for production use.</p> <ol> <li>Make an account at accounts.datajoint.io.</li> <li>In a workflow directory, make a config <code>json</code> file called    <code>dj_local_conf.json</code> using your DataJoint account information and     <code>tutorial-db.datajoint.io</code> as the host.     <pre><code>{\n    \"database.host\": \"tutorial-db.datajoint.io\",\n    \"database.user\": \"&lt;datajoint-username&gt;\",\n    \"database.password\": \"&lt;datajoint-password&gt;\",\n    \"loglevel\": \"INFO\",\n    \"safemode\": true,\n    \"display.limit\": 7,\n    \"display.width\": 14,\n    \"display.show_tuple_count\": true,\n    \"custom\": {\n    \"database.prefix\": \"&lt;datajoint-username_&gt;\"\n    }\n}\n</code></pre> Note: Your database prefix must begin with your username in order to have      permission to declare new tables.</li> <li>Launch a Python terminal and start interacting with the workflow.</li> </ol>","location":"elements/user-guide/#first-time"},{"title":"Local Database","text":"<ol> <li> <p>Install Docker.     Why Docker? Click for details.          Docker makes it easy to package a program, including the file system and related          code libraries, in a container. This container can be distributed to any          machine, both automating and standardizing the setup process.     </p> </li> <li> <p>Test that docker has been installed by running the following command:    <pre><code>docker run --rm hello-world\n</code></pre></p> </li> <li>Launch the DataJoint MySQL server with the following command:    <pre><code> docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=tutorial datajoint/mysql\n</code></pre>  What's this doing? Click for details. <ul> <li>Download a container image called datajoint/mysql, which is pre-installed and          configured MySQL database with appropriate settings for use with DataJoint     </li> <li>Open up the port 3306 (MySQL default) on your computer so that your database          server can accept connections.     </li> <li>Set the password for the root database user to be tutorial, which are then used          in the config file.     </li> </ul>  </li> <li>In a workflow directory, make a config <code>json</code> file called    <code>dj_local_conf.json</code> using the following details. The prefix can be set to any value.     <pre><code>{\n    \"database.host\": \"localhost\",\n    \"database.password\": \"tutorial\",\n    \"database.user\": \"root\",\n    \"database.port\": 3306,\n    \"loglevel\": \"INFO\",\n    \"safemode\": true,\n    \"display.limit\": 7,\n    \"display.width\": 14,\n    \"display.show_tuple_count\": true,\n    \"custom\": {\n        \"database.prefix\": \"neuro_\"\n    }\n}\n</code></pre></li> </ol>  Already familiar with Docker? Click here for details. <p>This document is written to apply to all example workflows. Many have a docker  folder used by developers to set up both a database and a local environment for  integration tests. Simply <code>docker compose up</code> the relevant file and  <code>docker exec</code> into the relevant container.</p>","location":"elements/user-guide/#local-database"},{"title":"Central Database","text":"<p>To set up a detabase on dedicated hardware may require expertise to set up and maintain. DataJoint's MySQL Docker image project  provides all the informaiton required to set up a dedicated database.</p>","location":"elements/user-guide/#central-database"},{"title":"Interacting with the Workflow","text":"","location":"elements/user-guide/#interacting-with-the-workflow"},{"title":"In Python","text":"<ol> <li> <p>Connect to the database and import tables</p> <pre><code>from &lt;relevant-workflow&gt;.pipeline import *\n</code></pre> </li> <li> <p>View the declared tables. For a more in depth explanation of how to run the workflow     and explore the data, refer to the      Jupyter notebooks      in the workflow directory.       Array Ephys: Click to expand details <pre><code>subject.Subject()\nsession.Session()\nephys.ProbeInsertion()\nephys.EphysRecording()\nephys.Clustering()\nephys.Clustering.Unit()\n</code></pre>   Calcium Imaging: Click to expand details <pre><code>subject.Subject()\nsession.Session()\nscan.Scan()\nscan.ScanInfo()\nimaging.ProcessingParamSet()\nimaging.ProcessingTask()\n</code></pre>   DeepLabCut: Click to expand details <pre><code>subject.Subject()\nsession.Session()\ntrain.TrainingTask()\nmodel.VideoRecording.File()\nmodel.Model()\nmodel.PoseEstimation.BodyPartPosition()\n</code></pre> </p> </li> </ol>","location":"elements/user-guide/#in-python"},{"title":"DataJoint LabBook","text":"<p>DataJoint LabBook is a graphical user interface to facilitate data entry for existing DataJoint tables.</p> <ul> <li>Labbook Website - If a database is public (e.g.,      <code>tutorial-db</code>) and you have access, you can view the contents here.</li> </ul> <ul> <li>DataJoint LabBook Documentation,      including prerequisites, installation, and running the application</li> </ul> <ul> <li>DataJoint LabBook GitHub Repository</li> </ul>","location":"elements/user-guide/#datajoint-labbook"},{"title":"Guidelines for Adoption","text":"<p>You have several options for adopting DataJoint workflows for your own experiments.</p>","location":"elements/management/adopt/"},{"title":"Adopt independently","text":"<p>DataJoint Elements are designed for adoption by independent users with moderate software development skills, good understanding of DataJoint principles, and adequate IT expertise or support.</p> <p>If you have not yet used DataJoint, we recommend completing our online training tutorials or attending a workshop either online or in person.  Interactive tutorials can be found on DataJoint CodeBook.</p>","location":"elements/management/adopt/#adopt-independently"},{"title":"Support from DataJoint","text":"<p>Our team provides support to labs to adopt DataJoint workflows in their research.</p> <p>This includes:</p> <ul> <li>User training</li> <li>Developer training</li> <li>Data and computation hosting<ul> <li>on your premises</li> <li>using your own cloud accounts</li> <li>fully managed cloud hosting by DataJoint</li> </ul> </li> <li>Workflow execution<ul> <li>configuration and automation</li> <li>optional fully managed service by DataJoint</li> </ul> </li> <li>Interfaces for data entry, export and publishing</li> </ul> <p>These services may be subsidized by grant funding for qualified research groups.</p>","location":"elements/management/adopt/#support-from-datajoint"},{"title":"Project Governance","text":"","location":"elements/management/governance/"},{"title":"Funding","text":"<p>This Resource is supported by the National Institute Of Neurological Disorders And Stroke of the National Institutes of Health under Award Number U24NS116470. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p>","location":"elements/management/governance/#funding"},{"title":"Scientific Steering Group","text":"<p>The project oversight and guidance is provided by the Scientific Steering Group comprising</p> <ul> <li>Mackenzie Mathis (EPFL)</li> <li>John Cunningham (Columbia U)</li> <li>Carlos Brody (Princeton U)</li> <li>Karel Svoboda (Allen Institute)</li> <li>Nick Steinmetz (U of Washington)</li> <li>Loren Frank (UCSF)</li> </ul>","location":"elements/management/governance/#scientific-steering-group"},{"title":"Outreach Plan","text":"<p>Broad engagement with the neuroscience community is necessary for the optimization, integration, and adoption of the Resource components.</p> <p>We conduct five types of outreach activities that require different approaches:</p>","location":"elements/management/outreach/"},{"title":"1. Precursor Projects","text":"<p>Our Selection Process requires a \"Precursor Project\" for any new experiment modality to be included in DataJoint Elements. A precursor project is a project that develops a DataJoint pipeline for its own experiments either independently or in collaboration with our team. We reach out to teams who develop DataJoint pipelines for new experiment paradigms and modalities to identify essential design motifs, analysis tools, and related tools and interfaces. We interview the core team to learn about their collaborative culture, practices, and procedures. We jointly review their open-source code and their plans for dissemination. In many cases, our team already collaborates with such teams through our other projects and we have a good understanding of their process. As we develop a new Element to support the new modality, we remain in contact with the team to include their contribution, solicit feedback, and evaluate design tradeoffs. When the new Element is released, a full attribution is given to the Precursor Project.</p> <p>Rationale: The Resource does not aim to develop fundamentally new solutions for   neurophysiology data acquisition and analysis. Rather it aims to systematize and   disseminate existing open-source tools proven in leading research projects.</p>","location":"elements/management/outreach/#1-precursor-projects"},{"title":"2. Tool Developers","text":"<p>DataJoint pipelines rely on analysis tools, atlases, data standards, archives and catalogs, and other neuroinformatics resources developed and maintained by the broader scientific community. To ensure sustainability of the Resource, we reach out to the tool developer to establish joint sustainability roadmaps.</p>","location":"elements/management/outreach/#2-tool-developers"},{"title":"3. Dissemination","text":"<p>We conduct activities to disseminate Resource components for adoption in diverse neuroscience labs. These activities include</p> <ul> <li>A central website for the Resource.</li> <li>Conference talks, presentations, and workshops</li> <li>Publications in peer-reviewed journals</li> <li>White papers posted on internet resources and websites</li> <li>On-site workshops by invitation</li> <li>Remote workshops and webinars</li> <li>Online interactive tutorials hosted on DataJoint CodeBook.</li> </ul>","location":"elements/management/outreach/#3-dissemination"},{"title":"4. Census","text":"<p>In order to measure the effectiveness of the Resource, we conduct several activities to estimate the adoption and use of the Resource components:</p> <ul> <li>A citation mechanism for individual components of the Resource.   Resource RRID     DataJoint RRID:SCR_014543   DataJoint Elements RRID:SCR_021894    </li> </ul> <ul> <li>Collect summary statistics of the number of downloads and repository forking.</li> </ul> <ul> <li>A register for self-reporting for component adoption and use (see DataJoint Census).</li> </ul>","location":"elements/management/outreach/#4-census"},{"title":"Management Plan","text":"<p>DataJoint Elements has established a Resource Management Plan to select projects for development, to assure quality, and to disseminate its output as summarized in the figure below:</p> <p></p> <p>The following sections provide detailed information.</p> <ul> <li>Team</li> <li>Project Governance</li> <li>Project Selection Process</li> <li>Quality Assurance</li> <li>Contribution Guideline</li> <li>Outreach Plan</li> <li>Licenses and User Agreements</li> </ul>","location":"elements/management/plan/"},{"title":"Quality Assurance","text":"<p>DataJoint and DataJoint Elements serve as a framework and starting points for numerous new projects, setting the standard of quality for data architecture and software design. To ensure higher quality, the following policies have been adopted into the software development lifecycle (SDLC).</p>","location":"elements/management/quality-assurance/"},{"title":"Coding Standards","text":"<p>When writing code, the following principles should be observed.</p> <ul> <li>Style: Code shall be written for clear readability. Uniform and clear naming     conventions, module structure, and formatting requirements shall be established     across all components of the project. Python's      PEP8 standard offers     clear guidance to this regard which can similarly be applied to all languages.<ul> <li>Python code is formatted with the black code formatter.</li> <li>Line length should be a maximum of 88 characters.</li> </ul> </li> </ul> <ul> <li>Maintenance Overhead: Code base size should be noted to prevent large,     unnecessarily complex solutions from being introduced. The idea is that the larger     the code base, the more there is to review and maintain. Therefore, we should aim     to find a compromise where we can keep the code base from becoming too large     without adding convoluted complexity.</li> </ul> <ul> <li>Performance: Performance drawbacks should be avoided, controlled, or, at least, be     properly monitored and justified. For instance: memory management, garbage     collection, disk reads/writes, and processing overhead should be regarded to ensure     that an efficient solution is achieved.</li> </ul>","location":"elements/management/quality-assurance/#coding-standards"},{"title":"Automated Testing","text":"<p>All components and their revisions must include appropriate automated software testing to be considered for release. The core framework must undergo thorough performance evaluation and comprehensive integration testing.</p> <p>Generally, this includes tests related to:</p> <ul> <li>Syntax: Verify that the code base does not contain any syntax errors and will run     or compile successfully.</li> </ul> <ul> <li>Unit &amp; Integration: Verify that low-level, method-specific tests (unit tests) and     any tests related coordinated interface between methods (integration tests) pass     successfully. Typically, when bugs are patched or features are introduced, unit and     integration tests are added to ensure that the use-case intended to be satisfied is     accounted for. This helps us prevent any regression in functionality.</li> </ul> <ul> <li>Style: Verify that the code base adheres to style guides for optimal readability.</li> </ul> <ul> <li>Code Coverage: Verify that the code base has similar or better code coverage than     the last run.</li> </ul>","location":"elements/management/quality-assurance/#automated-testing"},{"title":"Code Reviews","text":"<p>When introducing new code to the code base, the following will be required for acceptance by DataJoint core team into the main code repository.</p> <ul> <li>Independence: Proposed changes should not directly alter the code base in the   review process. New changes should be applied separately on a copy of the code base   and proposed for review by the DataJoint core team. For example, apply changes on a   GitHub fork and open a pull request targeting the <code>main</code> branch once ready for review.</li> </ul> <ul> <li>Etiquette: An author who has requested for a code for review should not accept and     merge their own code to the code base. A reviewer should not commit any suggestions     directly to the authors proposed changes but rather should allow the author to     review.</li> </ul> <ul> <li>Coding Standards: Ensure the above coding standards are respected.</li> </ul> <ul> <li>Summary: A description should be included that summarizes and highlights the     notable changes that are being proposed.</li> </ul> <ul> <li>Issue Reference: Any bugs or feature requests that have been filed in the issue     tracker that would be resolved by acceptance should be properly linked and     referenced.</li> </ul> <ul> <li>Satisfy Automated Tests: All automated tests associated with the project will be   verified to be successful prior to acceptance.</li> </ul> <ul> <li>Documentation: Documentation should be included to reflect any new feature or     behavior introduced.</li> </ul> <ul> <li>Release Notes: Include necessary updates to the release notes or change log to     capture a summary of the patched bugs and new feature introduction. Proper linking     should be maintained to associated tickets in issue tracker and reviews.</li> </ul>","location":"elements/management/quality-assurance/#code-reviews"},{"title":"Release Process","text":"<p>Upon satisfactory adherence to the above Coding Standards, Automated Testing, and Code Reviews:</p> <ul> <li>The package version will be incremented following the standard definition of   Semantic Versioning with a <code>Major.Minor.Patch</code>   number.</li> </ul> <ul> <li>Updates will be merged into the base repository <code>main</code> branch.</li> </ul> <ul> <li>A new release will be made on PyPI.</li> </ul> <p>For external research teams that reach out to us, we will provide engineering support to help users adopt the updated software, collect feedback, and resolve issues following the processes described in the section below. If the updates require changes in the design of the database schema or formats, a process for data migration will be provided upon request.</p>","location":"elements/management/quality-assurance/#release-process"},{"title":"User Feedback &amp; Issue Tracking","text":"<p>All components will be organized in GitHub repositories with guidelines for contribution, feedback, and issue submission to the issue tracker. For more information on the general policy around issue filing, tracking, and escalation, see the DataJoint Open-Source Contribute policy. For research groups that reach out to us, our team will work closely to collect feedback and resolve issues. Typically issues will be prioritized based on their criticality and impact. If new feature requirements become apparent, this may trigger the creation of a separate workflow or a major revision of an existing workflow.</p>","location":"elements/management/quality-assurance/#user-feedback-issue-tracking"},{"title":"Project Selection Process","text":"<p>The project milestones are set annually by the team under the stewardship of the NIH programmatic staff and with the guidance of the project's  Scientific Steering Group</p> <p>We have adopted the following general criteria for selecting and accepting new projects to be included in the Resource.</p> <ol> <li> <p>Open Precursor Projects</p> <p>At least one open-source DataJoint-based precursor project must exist for any new  experiment modality to be accepted for support as part of the Resource. The  precursor project team must be open to interviews to describe in detail their  process for the experiment workflow, tools, and interfaces.</p> <p>The precursor projects must provide sample data for testing during development and  for tutorials. The precursor projects will be acknowledged in the development of the  component.</p> <p>Rationale: This Resource does not aim to develop fundamentally new solutions for    neurophysiology data acquisition and analysis. Rather it seeks to systematize and    disseminate existing open-source tools proven in leading research projects.</p> </li> <li> <p>Impact</p> <p>New components proposed for support in the project must be shown to be in demand by a  substantial population or research groups, on the order of 100+ labs globally.</p> </li> <li> <p>Sustainability</p> <p>For all third-party tools or resources included in the proposed component, their  long-term maintenance roadmap must be established. When possible, we will contact  the developer team and work with them to establish a sustainability roadmap. If no  such roadmap can be established, alternative tools and resources must be identified  as replacement.</p> </li> </ol>","location":"elements/management/selection/"},{"title":"Team","text":"<p>The project is performed by DataJoint with Dimitri Yatsenko as Principal Investigator.</p>","location":"elements/management/team/"},{"title":"Scientists","text":"<ul> <li>Dimitri Yatsenko - PI</li> <li>Thinh Nguyen - Data Scientist</li> <li>Kabilar Gunalan - Data Scientist</li> <li>Joseph Burling - Data Scientist</li> <li>Chris Brozdowski - Data Scientist</li> <li>Tolga Dincer - Data Scientist</li> <li>Sid Hulyalkar - Data Scientist</li> <li>Jaerong Ahn - Data Scientist</li> <li>Kushal Bakshi - Data Scientist</li> </ul>","location":"elements/management/team/#scientists"},{"title":"Engineers","text":"<ul> <li>Raphael Guzman - Software Engineer</li> <li>Drew Yang - Data Systems Engineer</li> <li>Jeroen Verswijver - Software Engineer</li> <li>Adib Baji - Software Engineer</li> <li>Timothy Chandler - Data Systems Engineer</li> <li>Chetana Pitani - Software Engineer</li> </ul>","location":"elements/management/team/#engineers"},{"title":"Past contributors","text":"<ul> <li>Edgar Y. Walker - System architect, Data Scientist, Project Manager  (from project start to Jan, 2021)</li> <li>Andreas S. Tolias - Grant proposal contributor</li> <li>Jacob Reimer - Grant proposal contributor</li> <li>Shan Shen - Data Scientist</li> <li>Maho Sasaki - Software Engineer</li> <li>Daniel Sitonic - Software Engineer</li> <li>Christopher Turner - Data Systems Engineer</li> <li>David Godinez - Data Engineer</li> <li>Geetika Singh - Data Engineer</li> <li>Carlos Ortiz - Software Engineer</li> </ul> <p>The first-person pronouns \"we\" and \"our\" in these documents refer to those listed above.</p>","location":"elements/management/team/#past-contributors"},{"title":"External contributors","text":"<p>The principal components of the Resource are developed and distributed as open-source projects and external contributions are welcome. We have adopted a  Contribution Guide for DataJoint, DataJoint Elements, and related open-source tools.</p>","location":"elements/management/team/#external-contributors"},{"title":"Works","text":"<p>Docs coming soon!</p>","location":"works/"}]}